<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ELECTRA &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../../" src="../../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">ELECTRA</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/model_doc/electra.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="electra">
<h1>ELECTRA<a class="headerlink" href="#electra" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The ELECTRA model was proposed in the paper <a class="reference external" href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than
Generators</a>. ELECTRA is a new pretraining approach which trains two
transformer models: the generator and the discriminator. The generator’s role is to replace tokens in a sequence, and
is therefore trained as a masked language model. The discriminator, which is the model we’re interested in, tries to
identify which tokens were replaced by the generator in the sequence.</p>
<p>The abstract from the paper is the following:</p>
<p><em>Masked language modeling (MLM) pretraining methods such as BERT corrupt the input by replacing some tokens with [MASK]
and then train a model to reconstruct the original tokens. While they produce good results when transferred to
downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a
more sample-efficient pretraining task called replaced token detection. Instead of masking the input, our approach
corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead
of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that
predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments
demonstrate this new pretraining task is more efficient than MLM because the task is defined over all input tokens
rather than just the small subset that was masked out. As a result, the contextual representations learned by our
approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are
particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained
using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale,
where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when
using the same amount of compute.</em></p>
<p>Tips:</p>
<ul class="simple">
<li><p>ELECTRA is the pretraining approach, therefore there is nearly no changes done to the underlying model: BERT. The
only change is the separation of the embedding size and the hidden size: the embedding size is generally smaller,
while the hidden size is larger. An additional projection layer (linear) is used to project the embeddings from their
embedding size to the hidden size. In the case where the embedding size is the same as the hidden size, no projection
layer is used.</p></li>
<li><p>The ELECTRA checkpoints saved using <a class="reference external" href="https://github.com/google-research/electra">Google Research’s implementation</a>
contain both the generator and discriminator. The conversion script requires the user to name which model to export
into the correct architecture. Once converted to the HuggingFace format, these checkpoints may be loaded into all
available ELECTRA models, however. This means that the discriminator may be loaded in the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForMaskedLM</span></code> model, and the generator may be loaded in the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForPreTraining</span></code> model (the classification head will be randomly initialized as it
doesn’t exist in the generator).</p></li>
</ul>
<p>The original code can be found <a class="reference external" href="https://github.com/google-research/electra">here</a>.</p>
</div>
<div class="section" id="electraconfig">
<h2>ElectraConfig<a class="headerlink" href="#electraconfig" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electratokenizer">
<h2>ElectraTokenizer<a class="headerlink" href="#electratokenizer" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electratokenizerfast">
<h2>ElectraTokenizerFast<a class="headerlink" href="#electratokenizerfast" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electra-specific-outputs">
<h2>Electra specific outputs<a class="headerlink" href="#electra-specific-outputs" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electramodel">
<h2>ElectraModel<a class="headerlink" href="#electramodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electraforpretraining">
<h2>ElectraForPreTraining<a class="headerlink" href="#electraforpretraining" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electraformaskedlm">
<h2>ElectraForMaskedLM<a class="headerlink" href="#electraformaskedlm" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electraforsequenceclassification">
<h2>ElectraForSequenceClassification<a class="headerlink" href="#electraforsequenceclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electraformultiplechoice">
<h2>ElectraForMultipleChoice<a class="headerlink" href="#electraformultiplechoice" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electrafortokenclassification">
<h2>ElectraForTokenClassification<a class="headerlink" href="#electrafortokenclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="electraforquestionanswering">
<h2>ElectraForQuestionAnswering<a class="headerlink" href="#electraforquestionanswering" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectramodel">
<h2>TFElectraModel<a class="headerlink" href="#tfelectramodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectraforpretraining">
<h2>TFElectraForPreTraining<a class="headerlink" href="#tfelectraforpretraining" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectraformaskedlm">
<h2>TFElectraForMaskedLM<a class="headerlink" href="#tfelectraformaskedlm" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectraforsequenceclassification">
<h2>TFElectraForSequenceClassification<a class="headerlink" href="#tfelectraforsequenceclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectraformultiplechoice">
<h2>TFElectraForMultipleChoice<a class="headerlink" href="#tfelectraformultiplechoice" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectrafortokenclassification">
<h2>TFElectraForTokenClassification<a class="headerlink" href="#tfelectrafortokenclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfelectraforquestionanswering">
<h2>TFElectraForQuestionAnswering<a class="headerlink" href="#tfelectraforquestionanswering" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>