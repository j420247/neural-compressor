<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reformer &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../../" src="../../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Reformer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/model_doc/reformer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="reformer">
<h1>Reformer<a class="headerlink" href="#reformer" title="Permalink to this headline">¶</a></h1>
<p><strong>DISCLAIMER:</strong> This model is still a work in progress, if you see something strange, file a <a class="reference external" href="https://github.com/huggingface/transformers/issues/new?assignees=&amp;labels=&amp;template=bug-report.md&amp;title">Github Issue</a>.</p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The Reformer model was proposed in the paper <a class="reference external" href="https://arxiv.org/abs/2001.04451.pdf">Reformer: The Efficient Transformer</a> by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.</p>
<p>The abstract from the paper is the following:</p>
<p><em>Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.</em></p>
<p>The Authors’ code can be found <a class="reference external" href="https://github.com/google/trax/tree/master/trax/models/reformer">here</a>.</p>
</div>
<div class="section" id="axial-positional-encodings">
<h2>Axial Positional Encodings<a class="headerlink" href="#axial-positional-encodings" title="Permalink to this headline">¶</a></h2>
<p>Axial Positional Encodings were first implemented in Google’s <a class="reference external" href="https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29">trax library</a>
and developed by the authors of this model’s paper. In models that are treating very long input sequences, the
conventional position id encodings store an embedings vector of size <span class="math notranslate nohighlight">\(d\)</span> being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.hidden_size</span></code> for
every position <span class="math notranslate nohighlight">\(i, \ldots, n_s\)</span>, with <span class="math notranslate nohighlight">\(n_s\)</span> being <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.max_embedding_size</span></code>. This means that having
a sequence length of <span class="math notranslate nohighlight">\(n_s = 2^{19} \approx 0.5M\)</span> and a <code class="docutils literal notranslate"><span class="pre">config.hidden_size</span></code> of <span class="math notranslate nohighlight">\(d = 2^{10} \approx 1000\)</span>
would result in a position encoding matrix:</p>
<div class="math notranslate nohighlight">
\[X_{i,j}, \text{ with } i \in \left[1,\ldots, d\right] \text{ and } j \in \left[1,\ldots, n_s\right]\]</div>
<p>which alone has over 500M parameters to store. Axial positional encodings factorize <span class="math notranslate nohighlight">\(X_{i,j}\)</span> into two matrices:</p>
<div class="math notranslate nohighlight">
\[X^{1}_{i,j}, \text{ with } i \in \left[1,\ldots, d^1\right] \text{ and } j \in \left[1,\ldots, n_s^1\right]\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[X^{2}_{i,j}, \text{ with } i \in \left[1,\ldots, d^2\right] \text{ and } j \in \left[1,\ldots, n_s^2\right]\]</div>
<p>with:</p>
<div class="math notranslate nohighlight">
\[d = d^1 + d^2 \text{ and } n_s = n_s^1 \times n_s^2 .\]</div>
<p>Therefore the following holds:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X_{i,j} = \begin{cases}
            X^{1}_{i, k}, &amp; \text{if }\ i &lt; d^1 \text{ with } k = j \mod n_s^1 \\
            X^{2}_{i - d^1, l}, &amp; \text{if } i \ge d^1 \text{ with } l = \lfloor\frac{j}{n_s^1}\rfloor
          \end{cases}\end{split}\]</div>
<p>Intuitively, this means that a position embedding vector <span class="math notranslate nohighlight">\(x_j \in \mathbb{R}^{d}\)</span> is now the composition of two
factorized embedding vectors: <span class="math notranslate nohighlight">\(x^1_{k, l} + x^2_{l, k}\)</span>, where as the <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.max_embedding_size</span></code> dimension
<span class="math notranslate nohighlight">\(j\)</span> is factorized into <span class="math notranslate nohighlight">\(k \text{ and } l\)</span>. This design ensures that each position embedding vector
<span class="math notranslate nohighlight">\(x_j\)</span> is unique.</p>
<p>Using the above example again, axial position encoding with <span class="math notranslate nohighlight">\(d^1 = 2^5, d^2 = 2^5, n_s^1 = 2^9, n_s^2 = 2^{10}\)</span>
can drastically reduced the number of parameters to <span class="math notranslate nohighlight">\(2^{14} + 2^{15} \approx 49000\)</span> parameters.</p>
<p>In practice, the parameter <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.axial_pos_embds_dim</span></code> is set to a tuple <span class="math notranslate nohighlight">\((d^1, d^2)\)</span> which sum has to be
equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.hidden_size</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.axial_pos_shape</span></code> is set to a tuple <span class="math notranslate nohighlight">\((n_s^1, n_s^2)\)</span> which
product has to be equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.max_embedding_size</span></code>, which during training has to be equal to the <cite>sequence
length</cite> of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code>.</p>
</div>
<div class="section" id="lsh-self-attention">
<h2>LSH Self Attention<a class="headerlink" href="#lsh-self-attention" title="Permalink to this headline">¶</a></h2>
<p>In Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key
query embedding vectors are also tied. LSH self attention uses the locality sensitive hashing mechanism proposed in
<a class="reference external" href="https://arxiv.org/abs/1509.02897">Practical and Optimal LSH for Angular Distance</a> to assign each of the tied key
query embedding vectors to one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_buckets</span></code> possible buckets. The premise is that the more “similar”
key query embedding vectors (in terms of <em>cosine similarity</em>) are to each other, the more likely they are assigned to
the same bucket.</p>
<p>The accuracy of the LSH mechanism can be improved by increasing <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_hashes</span></code> or directly the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_hashes</span></code> of the forward function so that the output of the LSH self attention better approximates the output
of the “normal” full self attention. The buckets are then sorted and chunked into query key embedding vector chunks
each of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.lsh_chunk_length</span></code>. For each chunk, the query embedding vectors attend to its key vectors
(which are tied to themselves) and to the key embedding vectors of <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.lsh_num_chunks_before</span></code> previous
neighboring chunks and <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.lsh_num_chunks_after</span></code> following neighboring chunks.</p>
<p>For more information, see the <a class="reference external" href="https://arxiv.org/abs/2001.04451">original Paper</a> or this great <a class="reference external" href="https://www.pragmatic.ml/reformer-deep-dive/">blog post</a>.</p>
<p>Note that <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_buckets</span></code> can also be factorized into a list <span class="math notranslate nohighlight">\((n_{\text{buckets}}^1,
n_{\text{buckets}}^2)\)</span>. This way instead of assigning the query key embedding vectors to one of <span class="math notranslate nohighlight">\((1,\ldots,
n_{\text{buckets}})\)</span> they are assigned to one of <span class="math notranslate nohighlight">\((1-1,\ldots, n_{\text{buckets}}^1-1, \ldots,
1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)\)</span>. This is crucial for very long sequences to
save memory.</p>
<p>When training a model from scratch, it is recommended to leave <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_buckets=None</span></code>, so that depending on the
sequence length a good value for <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_buckets</span></code> is calculated on the fly. This value will then automatically be
saved in the config and should be reused for inference.</p>
<p>Using LSH self attention, the memory and time complexity of the query-key matmul operation can be reduced from
<span class="math notranslate nohighlight">\(\mathcal{O}(n_s \times n_s)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(n_s \times \log(n_s))\)</span>, which usually represents the memory
and time bottleneck in a transformer model, with <span class="math notranslate nohighlight">\(n_s\)</span> being the sequence length.</p>
</div>
<div class="section" id="local-self-attention">
<h2>Local Self Attention<a class="headerlink" href="#local-self-attention" title="Permalink to this headline">¶</a></h2>
<p>Local self attention is essentially a “normal” self attention layer with key, query and value projections, but is
chunked so that in each chunk of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.local_chunk_length</span></code> the query embedding vectors only attends to
the key embedding vectors in its chunk and to the key embedding vectors of <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.local_num_chunks_before</span></code>
previous neighboring chunks and <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.local_num_chunks_after</span></code> following neighboring chunks.</p>
<p>Using Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from
<span class="math notranslate nohighlight">\(\mathcal{O}(n_s \times n_s)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(n_s \times \log(n_s))\)</span>, which usually represents the memory
and time bottleneck in a transformer model, with <span class="math notranslate nohighlight">\(n_s\)</span> being the sequence length.</p>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>During training, we must ensure that the sequence length is set to a value that can be divided by the least common
multiple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.lsh_chunk_length</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.local_chunk_length</span></code> and that the parameters of the Axial
Positional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can
easily be trained on sequences as long as 64000 tokens.</p>
<p>For training, the <code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerModelWithLMHead</span></code> should be used as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;This is a sentence from the training data&#39;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="reformerconfig">
<h2>ReformerConfig<a class="headerlink" href="#reformerconfig" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformertokenizer">
<h2>ReformerTokenizer<a class="headerlink" href="#reformertokenizer" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformertokenizerfast">
<h2>ReformerTokenizerFast<a class="headerlink" href="#reformertokenizerfast" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformermodel">
<h2>ReformerModel<a class="headerlink" href="#reformermodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformermodelwithlmhead">
<h2>ReformerModelWithLMHead<a class="headerlink" href="#reformermodelwithlmhead" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformerformaskedlm">
<h2>ReformerForMaskedLM<a class="headerlink" href="#reformerformaskedlm" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformerforsequenceclassification">
<h2>ReformerForSequenceClassification<a class="headerlink" href="#reformerforsequenceclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="reformerforquestionanswering">
<h2>ReformerForQuestionAnswering<a class="headerlink" href="#reformerforquestionanswering" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>