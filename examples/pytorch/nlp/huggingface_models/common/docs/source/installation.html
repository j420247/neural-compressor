<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Installation &mdash; IntelÂ® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> IntelÂ® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">IntelÂ® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">IntelÂ® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">IntelÂ® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Installation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/installation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">Â¶</a></h1>
<p>ðŸ¤— Transformers is tested on Python 3.6+, and PyTorch 1.1.0+ or TensorFlow 2.0+.</p>
<p>You should install ðŸ¤— Transformers in a <a class="reference external" href="https://docs.python.org/3/library/venv.html">virtual environment</a>. If youâ€™re
unfamiliar with Python virtual environments, check out the <a class="reference external" href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/">user guide</a>. Create a virtual environment with the version of Python youâ€™re going
to use and activate it.</p>
<p>Now, if you want to use ðŸ¤— Transformers, you can install it with pip. If youâ€™d like to play with the examples, you
must install it from source.</p>
<div class="section" id="installation-with-pip">
<h2>Installation with pip<a class="headerlink" href="#installation-with-pip" title="Permalink to this headline">Â¶</a></h2>
<p>First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to <a class="reference external" href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available">TensorFlow installation page</a>,
<a class="reference external" href="https://pytorch.org/get-started/locally/#start-locally">PyTorch installation page</a> and/or
<a class="reference external" href="https://github.com/google/flax#quick-install">Flax installation page</a>
regarding the specific install command for your platform.</p>
<p>When TensorFlow 2.0 and/or PyTorch has been installed, ðŸ¤— Transformers can be installed using pip as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers
</pre></div>
</div>
<p>Alternatively, for CPU-support only, you can install ðŸ¤— Transformers and PyTorch in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>torch<span class="o">]</span>
</pre></div>
</div>
<p>or ðŸ¤— Transformers and TensorFlow 2.0 in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>tf-cpu<span class="o">]</span>
</pre></div>
</div>
<p>or ðŸ¤— Transformers and Flax in one line with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install transformers<span class="o">[</span>flax<span class="o">]</span>
</pre></div>
</div>
<p>To check ðŸ¤— Transformers is properly installed, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -c <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;we love you&#39;))&quot;</span>
</pre></div>
</div>
<p>It should download a pretrained model then print something like</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[{</span><span class="s1">&#39;label&#39;</span>: <span class="s1">&#39;POSITIVE&#39;</span>, <span class="s1">&#39;score&#39;</span>: <span class="m">0</span>.9998704791069031<span class="o">}]</span>
</pre></div>
</div>
<p>(Note that TensorFlow will print additional stuff before that last statement.)</p>
</div>
<div class="section" id="installing-from-source">
<h2>Installing from source<a class="headerlink" href="#installing-from-source" title="Permalink to this headline">Â¶</a></h2>
<p>Here is how to quickly install <code class="docutils literal notranslate"><span class="pre">transformers</span></code> from source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install git+https://github.com/huggingface/transformers
</pre></div>
</div>
<p>Note that this will install not the latest released version, but the bleeding edge <code class="docutils literal notranslate"><span class="pre">master</span></code> version, which you may want to use in case a bug has been fixed since the last official release and a new release hasnâ€™t  been yet rolled out.</p>
<p>While we strive to keep <code class="docutils literal notranslate"><span class="pre">master</span></code> operational at all times, if you notice some issues, they usually get fixed within a few hours or a day and and youâ€™re more than welcome to help us detect any problems by opening an <a class="reference external" href="https://github.com/huggingface/transformers/issues">Issue</a> and this way, things will get fixed even sooner.</p>
<p>Again, you can run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -c <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I hate you&#39;))&quot;</span>
</pre></div>
</div>
<p>to check ðŸ¤— Transformers is properly installed.</p>
</div>
<div class="section" id="editable-install">
<h2>Editable install<a class="headerlink" href="#editable-install" title="Permalink to this headline">Â¶</a></h2>
<p>If you want to constantly use the bleeding edge <code class="docutils literal notranslate"><span class="pre">master</span></code> version of the source code, or if you want to contribute to the library and need to test the changes in the code youâ€™re making, you will need an editable install. This is done by cloning the repository and installing with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers.git
<span class="nb">cd</span> transformers
pip install -e .
</pre></div>
</div>
<p>This command performs a magical link between the folder you cloned the repository to and your python library paths, and itâ€™ll look inside this folder in addition to the normal library-wide paths. So if normally your python packages get installed into:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">~/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">7</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span>
</pre></div>
</div>
<p>now this editable install will reside where you clone the folder to, e.g. <code class="docutils literal notranslate"><span class="pre">~/transformers/</span></code> and python will search it too.</p>
<p>Do note that you have to keep that <code class="docutils literal notranslate"><span class="pre">transformers</span></code> folder around and not delete it to continue using the  <code class="docutils literal notranslate"><span class="pre">Transformers</span></code> library.</p>
<p>Now, letâ€™s get to the real benefit of this installation approach. Say, you saw some new feature has been just committed into <code class="docutils literal notranslate"><span class="pre">master</span></code>. If you have already performed all the steps above, to update your transformers to include all the latest commits, all you need to do is to <code class="docutils literal notranslate"><span class="pre">cd</span></code> into that cloned repository folder and update the clone to the latest version:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">~/</span><span class="n">transformers</span><span class="o">/</span>
<span class="n">git</span> <span class="n">pull</span>
</pre></div>
</div>
<p>There is nothing else to do. Your python environment will find the bleeding edge version of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> on the next run.</p>
</div>
<div class="section" id="with-conda">
<h2>With conda<a class="headerlink" href="#with-conda" title="Permalink to this headline">Â¶</a></h2>
<p>Since Transformers version v4.0.0, we now have a conda channel: <code class="docutils literal notranslate"><span class="pre">huggingface</span></code>.</p>
<p>ðŸ¤— Transformers can be installed using conda as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">huggingface</span> <span class="n">transformers</span>
</pre></div>
</div>
<p>Follow the installation pages of TensorFlow, PyTorch or Flax to see how to install them with conda.</p>
</div>
<div class="section" id="caching-models">
<h2>Caching models<a class="headerlink" href="#caching-models" title="Permalink to this headline">Â¶</a></h2>
<p>This library provides pretrained models that will be downloaded and cached locally. Unless you specify a location with
<code class="docutils literal notranslate"><span class="pre">cache_dir=...</span></code> when you use methods like <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code>, these models will automatically be downloaded in the
folder given by the shell environment variable <code class="docutils literal notranslate"><span class="pre">TRANSFORMERS_CACHE</span></code>. The default value for it will be the Hugging
Face cache home followed by <code class="docutils literal notranslate"><span class="pre">/transformers/</span></code>. This is (by order of priority):</p>
<ul class="simple">
<li><p>shell environment variable <code class="docutils literal notranslate"><span class="pre">HF_HOME</span></code></p></li>
<li><p>shell environment variable <code class="docutils literal notranslate"><span class="pre">XDG_CACHE_HOME</span></code> + <code class="docutils literal notranslate"><span class="pre">/huggingface/</span></code></p></li>
<li><p>default: <code class="docutils literal notranslate"><span class="pre">~/.cache/huggingface/</span></code></p></li>
</ul>
<p>So if you donâ€™t have any specific environment variable set, the cache directory will be at
<code class="docutils literal notranslate"><span class="pre">~/.cache/huggingface/transformers/</span></code>.</p>
<p><strong>Note:</strong> If you have set a shell environment variable for one of the predecessors of this library
(<code class="docutils literal notranslate"><span class="pre">PYTORCH_TRANSFORMERS_CACHE</span></code> or <code class="docutils literal notranslate"><span class="pre">PYTORCH_PRETRAINED_BERT_CACHE</span></code>), those will be used if there is no shell
environment variable for <code class="docutils literal notranslate"><span class="pre">TRANSFORMERS_CACHE</span></code>.</p>
<div class="section" id="note-on-model-downloads-continuous-integration-or-large-scale-deployments">
<h3>Note on model downloads (Continuous Integration or large-scale deployments)<a class="headerlink" href="#note-on-model-downloads-continuous-integration-or-large-scale-deployments" title="Permalink to this headline">Â¶</a></h3>
<p>If you expect to be downloading large volumes of models (more than 1,000) from our hosted bucket (for instance through
your CI setup, or a large-scale production deployment), please cache the model files on your end. It will be way
faster, and cheaper. Feel free to contact us privately if you need any help.</p>
</div>
</div>
<div class="section" id="do-you-want-to-run-a-transformer-model-on-a-mobile-device">
<h2>Do you want to run a Transformer model on a mobile device?<a class="headerlink" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device" title="Permalink to this headline">Â¶</a></h2>
<p>You should check out our <a class="reference external" href="https://github.com/huggingface/swift-coreml-transformers">swift-coreml-transformers</a> repo.</p>
<p>It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains <code class="docutils literal notranslate"><span class="pre">GPT-2</span></code>,
<code class="docutils literal notranslate"><span class="pre">DistilGPT-2</span></code>, <code class="docutils literal notranslate"><span class="pre">BERT</span></code>, and <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code>) to CoreML models that run on iOS devices.</p>
<p>At some point in the future, youâ€™ll be able to seamlessly move from pretraining or fine-tuning models in PyTorch or
TensorFlow 2.0 to productizing them in CoreML, or prototype a model or an app in CoreML then research its
hyperparameters or architecture from PyTorch or TensorFlow 2.0. Super exciting!</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IntelÂ® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>