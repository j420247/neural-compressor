<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Fine-tuning with custom datasets &mdash; Intel¬Æ Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> Intel¬Æ Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">Intel¬Æ Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel¬Æ Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel¬Æ Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Fine-tuning with custom datasets</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/custom_datasets.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="fine-tuning-with-custom-datasets">
<h1>Fine-tuning with custom datasets<a class="headerlink" href="#fine-tuning-with-custom-datasets" title="Permalink to this headline">¬∂</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The datasets used in this tutorial are available and can be more easily accessed using the <a class="reference external" href="https://github.com/huggingface/nlp">ü§ó NLP library</a>. We do not use this library to access the datasets here since this tutorial
meant to illustrate how to work with your own data. A brief of introduction can be found at the end of the tutorial
in the section ‚Äú<a class="reference internal" href="#nlplib"><span class="std std-ref">Using the ü§ó NLP Datasets &amp; Metrics library</span></a>‚Äù.</p>
</div>
<p>This tutorial will take you through several examples of using ü§ó Transformers models with your own datasets. The guide
shows one of many valid workflows for using these models and is meant to be illustrative rather than definitive. We
show examples of reading in several data formats, preprocessing the data for several types of tasks, and then preparing
the data into PyTorch/TensorFlow <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> objects which can easily be used either with
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code> or with native PyTorch/TensorFlow.</p>
<p>We include several examples, each of which demonstrates a different type of common downstream task:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#seq-imdb"><span class="std std-ref">Sequence Classification with IMDb Reviews</span></a></p></li>
<li><p><a class="reference internal" href="#tok-ner"><span class="std std-ref">Token Classification with W-NUT Emerging Entities</span></a></p></li>
<li><p><a class="reference internal" href="#qa-squad"><span class="std std-ref">Question Answering with SQuAD 2.0</span></a></p></li>
<li><p><a class="reference internal" href="#resources"><span class="std std-ref">Additional Resources</span></a></p></li>
</ul>
</div></blockquote>
<div class="section" id="sequence-classification-with-imdb-reviews">
<span id="seq-imdb"></span><h2>Sequence Classification with IMDb Reviews<a class="headerlink" href="#sequence-classification-with-imdb-reviews" title="Permalink to this headline">¬∂</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset can be explored in the Hugging Face model hub (<a class="reference external" href="https://huggingface.co/datasets/imdb">IMDb</a>), and
can be alternatively downloaded with the ü§ó NLP library with <code class="docutils literal notranslate"><span class="pre">load_dataset(&quot;imdb&quot;)</span></code>.</p>
</div>
<p>In this example, we‚Äôll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes
the text of a review and requires the model to predict whether the sentiment of the review is positive or negative.
Let‚Äôs start by downloading the dataset from the <a class="reference external" href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a> webpage.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar -xf aclImdb_v1.tar.gz
</pre></div>
</div>
<p>This data is organized into <code class="docutils literal notranslate"><span class="pre">pos</span></code> and <code class="docutils literal notranslate"><span class="pre">neg</span></code> folders with one text file per example. Let‚Äôs write a function that can
read this in.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">read_imdb_split</span><span class="p">(</span><span class="n">split_dir</span><span class="p">):</span>
    <span class="n">split_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">split_dir</span><span class="p">)</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">label_dir</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="s2">&quot;neg&quot;</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">text_file</span> <span class="ow">in</span> <span class="p">(</span><span class="n">split_dir</span><span class="o">/</span><span class="n">label_dir</span><span class="p">)</span><span class="o">.</span><span class="n">iterdir</span><span class="p">():</span>
            <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text_file</span><span class="o">.</span><span class="n">read_text</span><span class="p">())</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">label_dir</span> <span class="ow">is</span> <span class="s2">&quot;neg&quot;</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">texts</span><span class="p">,</span> <span class="n">labels</span>

<span class="n">train_texts</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">read_imdb_split</span><span class="p">(</span><span class="s1">&#39;aclImdb/train&#39;</span><span class="p">)</span>
<span class="n">test_texts</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">read_imdb_split</span><span class="p">(</span><span class="s1">&#39;aclImdb/test&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>We now have a train and test dataset, but let‚Äôs also also create a validation set which we can use for for evaluation
and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_texts</span><span class="p">,</span> <span class="n">val_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Alright, we‚Äôve read in our dataset. Now let‚Äôs tackle tokenization. We‚Äôll eventually train a classifier using
pre-trained DistilBert, so let‚Äôs use the DistilBert tokenizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizerFast</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can simply pass our texts to the tokenizer. We‚Äôll pass <code class="docutils literal notranslate"><span class="pre">truncation=True</span></code> and <code class="docutils literal notranslate"><span class="pre">padding=True</span></code>, which will
ensure that all of our sequences are padded to the same length and are truncated to be no longer model‚Äôs maximum input
length. This will allow us to feed batches of sequences into the model at the same time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_texts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">val_texts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">test_texts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, let‚Äôs turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> object and implementing <code class="docutils literal notranslate"><span class="pre">__len__</span></code> and <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>. In TensorFlow, we pass our input
encodings and labels to the <code class="docutils literal notranslate"><span class="pre">from_tensor_slices</span></code> constructor method. We put the data in this format so that the data
can be easily batched such that each key in the batch encoding corresponds to a named parameter of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method of the model we will train.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">IMDbDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encodings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="n">encodings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">IMDbDataset</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">IMDbDataset</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">IMDbDataset</span><span class="p">(</span><span class="n">test_encodings</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">),</span>
    <span class="n">train_labels</span>
<span class="p">))</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">),</span>
    <span class="n">val_labels</span>
<span class="p">))</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">test_encodings</span><span class="p">),</span>
    <span class="n">test_labels</span>
<span class="p">))</span>
</pre></div>
</div>
<p>Now that our datasets our ready, we can fine-tune a model either with the ü§ó
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code> or with native PyTorch/TensorFlow. See <a class="reference internal" href="training.html"><span class="doc">training</span></a>.</p>
<div class="section" id="fine-tuning-with-trainer">
<span id="ft-trainer"></span><h3>Fine-tuning with Trainer<a class="headerlink" href="#fine-tuning-with-trainer" title="Permalink to this headline">¬∂</a></h3>
<p>The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model
to fine-tune, define the <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainingArguments</span></code> and
instantiate a <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;./results&#39;</span><span class="p">,</span>          <span class="c1"># output directory</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>              <span class="c1"># total number of training epochs</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>  <span class="c1"># batch size per device during training</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>   <span class="c1"># batch size for evaluation</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>                <span class="c1"># number of warmup steps for learning rate scheduler</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>               <span class="c1"># strength of weight decay</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s1">&#39;./logs&#39;</span><span class="p">,</span>            <span class="c1"># directory for storing logs</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>                         <span class="c1"># the instantiated ü§ó Transformers model to be trained</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>                  <span class="c1"># training arguments, defined above</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>         <span class="c1"># training dataset</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">val_dataset</span>             <span class="c1"># evaluation dataset</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">TFTrainer</span><span class="p">,</span> <span class="n">TFTrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TFTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;./results&#39;</span><span class="p">,</span>          <span class="c1"># output directory</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>              <span class="c1"># total number of training epochs</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>  <span class="c1"># batch size per device during training</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>   <span class="c1"># batch size for evaluation</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>                <span class="c1"># number of warmup steps for learning rate scheduler</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>               <span class="c1"># strength of weight decay</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s1">&#39;./logs&#39;</span><span class="p">,</span>            <span class="c1"># directory for storing logs</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">with</span> <span class="n">training_args</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TFTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>                         <span class="c1"># the instantiated ü§ó Transformers model to be trained</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>                  <span class="c1"># training arguments, defined above</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>         <span class="c1"># training dataset</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">val_dataset</span>             <span class="c1"># evaluation dataset</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="fine-tuning-with-native-pytorch-tensorflow">
<span id="ft-native"></span><h3>Fine-tuning with native PyTorch/TensorFlow<a class="headerlink" href="#fine-tuning-with-native-pytorch-tensorflow" title="Permalink to this headline">¬∂</a></h3>
<p>We can also train use native PyTorch or TensorFlow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">AdamW</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">)</span> <span class="c1"># can also use any keras loss fn</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="token-classification-with-w-nut-emerging-entities">
<span id="tok-ner"></span><h2>Token Classification with W-NUT Emerging Entities<a class="headerlink" href="#token-classification-with-w-nut-emerging-entities" title="Permalink to this headline">¬∂</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset can be explored in the Hugging Face model hub (<a class="reference external" href="https://huggingface.co/datasets/wnut_17">WNUT-17</a>),
and can be alternatively downloaded with the ü§ó NLP library with <code class="docutils literal notranslate"><span class="pre">load_dataset(&quot;wnut_17&quot;)</span></code>.</p>
</div>
<p>Next we will look at token classification. Rather than classifying an entire sequence, this task classifies token by
token. We‚Äôll demonstrate how to do this with <a class="reference external" href="http://nlpprogress.com/english/named_entity_recognition.html">Named Entity Recognition</a>, which involves identifying tokens which correspond to
a predefined set of ‚Äúentities‚Äù. Specifically, we‚Äôll use the <a class="reference external" href="http://noisy-text.github.io/2017/emerging-rare-entities.html">W-NUT Emerging and Rare entities</a> corpus. The data is given as a collection of
pre-tokenized documents where each token is assigned a tag.</p>
<p>Let‚Äôs start by downloading the data.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget http://noisy-text.github.io/2017/files/wnut17train.conll
</pre></div>
</div>
<p>In this case, we‚Äôll just download the train set, which is a single text file. Each line of the file contains either (1)
a word and tag separated by a tab, or (2) a blank line indicating the end of a document. Let‚Äôs write a function to read
this in. We‚Äôll take in the file path and return <code class="docutils literal notranslate"><span class="pre">token_docs</span></code> which is a list of lists of token strings, and
<code class="docutils literal notranslate"><span class="pre">token_tags</span></code> which is a list of lists of tag strings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">read_wnut</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

    <span class="n">raw_text</span> <span class="o">=</span> <span class="n">file_path</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">raw_docs</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\n\t?\n&#39;</span><span class="p">,</span> <span class="n">raw_text</span><span class="p">)</span>
    <span class="n">token_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tag_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">raw_docs</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">):</span>
            <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
        <span class="n">token_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">tag_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_docs</span><span class="p">,</span> <span class="n">tag_docs</span>

<span class="n">texts</span><span class="p">,</span> <span class="n">tags</span> <span class="o">=</span> <span class="n">read_wnut</span><span class="p">(</span><span class="s1">&#39;wnut17train.conll&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Just to see what this data looks like, let‚Äôs take a look at a segment of the first document.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">10</span><span class="p">:</span><span class="mi">17</span><span class="p">],</span> <span class="n">tags</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">10</span><span class="p">:</span><span class="mi">17</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="go">[&#39;for&#39;, &#39;two&#39;, &#39;weeks&#39;, &#39;.&#39;, &#39;Empire&#39;, &#39;State&#39;, &#39;Building&#39;]</span>
<span class="go">[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-location&#39;, &#39;I-location&#39;, &#39;I-location&#39;]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">location</span></code> is an entity type, <code class="docutils literal notranslate"><span class="pre">B-</span></code> indicates the beginning of an entity, and <code class="docutils literal notranslate"><span class="pre">I-</span></code> indicates consecutive positions
of the same entity (‚ÄúEmpire State Building‚Äù is considered one entity). <code class="docutils literal notranslate"><span class="pre">O</span></code> indicates the token does not correspond to
any entity.</p>
<p>Now that we‚Äôve read the data in, let‚Äôs create a train/validation split:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_texts</span><span class="p">,</span> <span class="n">val_texts</span><span class="p">,</span> <span class="n">train_tags</span><span class="p">,</span> <span class="n">val_tags</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">tags</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, let‚Äôs create encodings for our tokens and tags. For the tags, we can start by just create a simple mapping which
we‚Äôll use in a moment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">unique_tags</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tag</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tags</span> <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
<span class="n">tag2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">tag</span><span class="p">:</span> <span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">unique_tags</span><span class="p">)}</span>
<span class="n">id2tag</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">:</span> <span class="n">tag</span> <span class="k">for</span> <span class="n">tag</span><span class="p">,</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tag2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
<p>To encode the tokens, we‚Äôll use a pre-trained DistilBert tokenizer. We can tell the tokenizer that we‚Äôre dealing with
ready-split tokens rather than full sentence strings by passing <code class="docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code>. We‚Äôll also pass
<code class="docutils literal notranslate"><span class="pre">padding=True</span></code> and <code class="docutils literal notranslate"><span class="pre">truncation=True</span></code> to pad the sequences to be the same length. Lastly, we can tell the model to
return information about the tokens which are split by the wordpiece tokenization process, which we will need in a
moment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizerFast</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">)</span>
<span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_texts</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">val_texts</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Great, so now our tokens are nicely encoded in the format that they need to be in to feed them into our DistilBert
model below.</p>
<p>Now we arrive at a common obstacle with using pre-trained models for token-level classification: many of the tokens in
the W-NUT corpus are not in DistilBert‚Äôs vocabulary. Bert and many models like it use a method called WordPiece
Tokenization, meaning that single words are split into multiple tokens such that each token is likely to be in the
vocabulary. For example, DistilBert‚Äôs tokenizer would split the Twitter handle <code class="docutils literal notranslate"><span class="pre">&#64;huggingface</span></code> into the tokens <code class="docutils literal notranslate"><span class="pre">['&#64;',</span>
<span class="pre">'hugging',</span> <span class="pre">'##face']</span></code>. This is a problem for us because we have exactly one tag per token. If the tokenizer splits a
token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels.</p>
<p>One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in ü§ó
Transformers by setting the labels we wish to ignore to <code class="docutils literal notranslate"><span class="pre">-100</span></code>. In the example above, if the label for
<code class="docutils literal notranslate"><span class="pre">&#64;HuggingFace</span></code> is <code class="docutils literal notranslate"><span class="pre">3</span></code> (indexing <code class="docutils literal notranslate"><span class="pre">B-corporation</span></code>), we would set the labels of <code class="docutils literal notranslate"><span class="pre">['&#64;',</span> <span class="pre">'hugging',</span> <span class="pre">'##face']</span></code> to
<code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">-100,</span> <span class="pre">-100]</span></code>.</p>
<p>Let‚Äôs write a function to do this. This is where we will use the <code class="docutils literal notranslate"><span class="pre">offset_mapping</span></code> from the tokenizer as mentioned
above. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token‚Äôs
start position and end position relative to the original token it was split from. That means that if the first position
in the tuple is anything other than <code class="docutils literal notranslate"><span class="pre">0</span></code>, we will set its corresponding label to <code class="docutils literal notranslate"><span class="pre">-100</span></code>. While we‚Äôre at it, we can
also set labels to <code class="docutils literal notranslate"><span class="pre">-100</span></code> if the second position of the offset mapping is <code class="docutils literal notranslate"><span class="pre">0</span></code>, since this means it must be a
special token like <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code> or <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to a recently fixed bug, -1 must be used instead of -100 when using TensorFlow in ü§ó Transformers &lt;= 3.02.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">encode_tags</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span> <span class="n">encodings</span><span class="p">):</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[[</span><span class="n">tag2id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>
    <span class="n">encoded_labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc_labels</span><span class="p">,</span> <span class="n">doc_offset</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">encodings</span><span class="o">.</span><span class="n">offset_mapping</span><span class="p">):</span>
        <span class="c1"># create an empty array of -100</span>
        <span class="n">doc_enc_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_offset</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mi">100</span>
        <span class="n">arr_offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">doc_offset</span><span class="p">)</span>

        <span class="c1"># set labels whose first offset position is 0 and the second is not 0</span>
        <span class="n">doc_enc_labels</span><span class="p">[(</span><span class="n">arr_offset</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">arr_offset</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="n">doc_labels</span>
        <span class="n">encoded_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc_enc_labels</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">encoded_labels</span>

<span class="n">train_labels</span> <span class="o">=</span> <span class="n">encode_tags</span><span class="p">(</span><span class="n">train_tags</span><span class="p">,</span> <span class="n">train_encodings</span><span class="p">)</span>
<span class="n">val_labels</span> <span class="o">=</span> <span class="n">encode_tags</span><span class="p">(</span><span class="n">val_tags</span><span class="p">,</span> <span class="n">val_encodings</span><span class="p">)</span>
</pre></div>
</div>
<p>The hard part is now done. Just as in the sequence classification example above, we can create a dataset object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">WNUTDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encodings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="n">encodings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="n">train_encodings</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">)</span> <span class="c1"># we don&#39;t want to pass this to the model</span>
<span class="n">val_encodings</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">WNUTDataset</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">WNUTDataset</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">train_encodings</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">)</span> <span class="c1"># we don&#39;t want to pass this to the model</span>
<span class="n">val_encodings</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;offset_mapping&quot;</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">),</span>
    <span class="n">train_labels</span>
<span class="p">))</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">),</span>
    <span class="n">val_labels</span>
<span class="p">))</span>
</pre></div>
</div>
<p>Now load in a token classification model and specify the number of labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertForTokenClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_tags</span><span class="p">))</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForTokenClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_tags</span><span class="p">))</span>
</pre></div>
</div>
<p>The data and model are both ready to go. You can train the model either with
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code> or with native PyTorch/TensorFlow, exactly as in the
sequence classification example above.</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#ft-trainer"><span class="std std-ref">Fine-tuning with Trainer</span></a></p></li>
<li><p><a class="reference internal" href="#ft-native"><span class="std std-ref">Fine-tuning with native PyTorch/TensorFlow</span></a></p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="question-answering-with-squad-2-0">
<span id="qa-squad"></span><h2>Question Answering with SQuAD 2.0<a class="headerlink" href="#question-answering-with-squad-2-0" title="Permalink to this headline">¬∂</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This dataset can be explored in the Hugging Face model hub (<a class="reference external" href="https://huggingface.co/datasets/squad_v2">SQuAD V2</a>), and can be alternatively downloaded with the ü§ó NLP library with
<code class="docutils literal notranslate"><span class="pre">load_dataset(&quot;squad_v2&quot;)</span></code>.</p>
</div>
<p>Question answering comes in many forms. In this example, we‚Äôll look at the particular type of extractive QA that
involves answering a question about a passage by highlighting the segment of the passage that answers the question.
This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the
<a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">Stanford Question Answering Dataset (SQuAD) 2.0</a>.</p>
<p>We will start by downloading the data:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir squad
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json
</pre></div>
</div>
<p>Each split is in a structured json file with a number of questions and answers for each passage (or context). We‚Äôll
take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since
there are multiple questions per context):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">read_squad</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">squad_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="n">contexts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">answers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">squad_dict</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">passage</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;paragraphs&#39;</span><span class="p">]:</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">passage</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">passage</span><span class="p">[</span><span class="s1">&#39;qas&#39;</span><span class="p">]:</span>
                <span class="n">question</span> <span class="o">=</span> <span class="n">qa</span><span class="p">[</span><span class="s1">&#39;question&#39;</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">qa</span><span class="p">[</span><span class="s1">&#39;answers&#39;</span><span class="p">]:</span>
                    <span class="n">contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
                    <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
                    <span class="n">answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">questions</span><span class="p">,</span> <span class="n">answers</span>

<span class="n">train_contexts</span><span class="p">,</span> <span class="n">train_questions</span><span class="p">,</span> <span class="n">train_answers</span> <span class="o">=</span> <span class="n">read_squad</span><span class="p">(</span><span class="s1">&#39;squad/train-v2.0.json&#39;</span><span class="p">)</span>
<span class="n">val_contexts</span><span class="p">,</span> <span class="n">val_questions</span><span class="p">,</span> <span class="n">val_answers</span> <span class="o">=</span> <span class="n">read_squad</span><span class="p">(</span><span class="s1">&#39;squad/dev-v2.0.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the
correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on
this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which <em>token</em> positions the
answer begins and ends.</p>
<p>First, let‚Äôs get the <em>character</em> position at which the answer ends in the passage (we are given the starting position).
Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_end_idx</span><span class="p">(</span><span class="n">answers</span><span class="p">,</span> <span class="n">contexts</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">answer</span><span class="p">,</span> <span class="n">context</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">answers</span><span class="p">,</span> <span class="n">contexts</span><span class="p">):</span>
        <span class="n">gold_text</span> <span class="o">=</span> <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_start&#39;</span><span class="p">]</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">gold_text</span><span class="p">)</span>

        <span class="c1"># sometimes squad answers are off by a character or two ‚Äì fix this</span>
        <span class="k">if</span> <span class="n">context</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">==</span> <span class="n">gold_text</span><span class="p">:</span>
            <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_end&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">end_idx</span>
        <span class="k">elif</span> <span class="n">context</span><span class="p">[</span><span class="n">start_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">end_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">gold_text</span><span class="p">:</span>
            <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_start&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_end&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">end_idx</span> <span class="o">-</span> <span class="mi">1</span>     <span class="c1"># When the gold label is off by one character</span>
        <span class="k">elif</span> <span class="n">context</span><span class="p">[</span><span class="n">start_idx</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="n">end_idx</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">gold_text</span><span class="p">:</span>
            <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_start&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="n">answer</span><span class="p">[</span><span class="s1">&#39;answer_end&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">end_idx</span> <span class="o">-</span> <span class="mi">2</span>     <span class="c1"># When the gold label is off by two characters</span>

<span class="n">add_end_idx</span><span class="p">(</span><span class="n">train_answers</span><span class="p">,</span> <span class="n">train_contexts</span><span class="p">)</span>
<span class="n">add_end_idx</span><span class="p">(</span><span class="n">val_answers</span><span class="p">,</span> <span class="n">val_contexts</span><span class="p">)</span>
</pre></div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">train_answers</span></code> and <code class="docutils literal notranslate"><span class="pre">val_answers</span></code> include the character end positions and the corrected start positions. Next,
let‚Äôs tokenize our context/question pairs. ü§ó Tokenizers can accept parallel lists of sequences and encode them together
as sequence pairs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizerFast</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_contexts</span><span class="p">,</span> <span class="n">train_questions</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">val_contexts</span><span class="p">,</span> <span class="n">val_questions</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we need to convert our character start/end positions to token start/end positions. When using ü§ó Fast Tokenizers,
we can use the built in <code class="xref py py-func docutils literal notranslate"><span class="pre">char_to_token()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_token_positions</span><span class="p">(</span><span class="n">encodings</span><span class="p">,</span> <span class="n">answers</span><span class="p">):</span>
    <span class="n">start_positions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">end_positions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">answers</span><span class="p">)):</span>
        <span class="n">start_positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encodings</span><span class="o">.</span><span class="n">char_to_token</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">answers</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;answer_start&#39;</span><span class="p">]))</span>
        <span class="n">end_positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encodings</span><span class="o">.</span><span class="n">char_to_token</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">answers</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;answer_end&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># if start position is None, the answer passage has been truncated</span>
        <span class="k">if</span> <span class="n">start_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">start_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
        <span class="k">if</span> <span class="n">end_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">end_positions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>

    <span class="n">encodings</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;start_positions&#39;</span><span class="p">:</span> <span class="n">start_positions</span><span class="p">,</span> <span class="s1">&#39;end_positions&#39;</span><span class="p">:</span> <span class="n">end_positions</span><span class="p">})</span>

<span class="n">add_token_positions</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">,</span> <span class="n">train_answers</span><span class="p">)</span>
<span class="n">add_token_positions</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">,</span> <span class="n">val_answers</span><span class="p">)</span>
</pre></div>
</div>
<p>Our data is ready. Let‚Äôs just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In
PyTorch, we define a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class. In TensorFlow, we pass a tuple of <code class="docutils literal notranslate"><span class="pre">(inputs_dict,</span> <span class="pre">labels_dict)</span></code> to the
<code class="docutils literal notranslate"><span class="pre">from_tensor_slices</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">SquadDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encodings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span> <span class="o">=</span> <span class="n">encodings</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encodings</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">SquadDataset</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">SquadDataset</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">)</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">train_encodings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">]},</span>
    <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">train_encodings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;start_positions&#39;</span><span class="p">,</span> <span class="s1">&#39;end_positions&#39;</span><span class="p">]}</span>
<span class="p">))</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span>
    <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val_encodings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">]},</span>
    <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val_encodings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;start_positions&#39;</span><span class="p">,</span> <span class="s1">&#39;end_positions&#39;</span><span class="p">]}</span>
<span class="p">))</span>
</pre></div>
</div>
<p>Now we can use a DistilBert model with a QA head for training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The data and model are both ready to go. You can train the model with
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code> exactly as in the sequence classification example
above. If using native PyTorch, replace <code class="docutils literal notranslate"><span class="pre">labels</span></code> with <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> and <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> in the training
example. If using Keras‚Äôs <code class="docutils literal notranslate"><span class="pre">fit</span></code>, we need to make a minor modification to handle this example since it involves
multiple model outputs.</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#ft-trainer"><span class="std std-ref">Fine-tuning with Trainer</span></a></p></li>
</ul>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">start_positions</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;start_positions&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">end_positions</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;end_positions&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span><span class="o">=</span><span class="n">end_positions</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="c1"># Keras will expect a tuple when dealing with labels</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;start_positions&#39;</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;end_positions&#39;</span><span class="p">])))</span>

<span class="c1"># Keras will assign a separate loss for each output and add them together. So we&#39;ll just use the standard CE loss</span>
<span class="c1"># instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.</span>
<span class="c1"># Note that this means the loss will be 2x of when using TFTrainer since we&#39;re adding instead of averaging them.</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">distilbert</span><span class="o">.</span><span class="n">return_dict</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># if using ü§ó Transformers &gt;3.02, make sure outputs are tuples</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># can also use any keras loss fn</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<span id="resources"></span><h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¬∂</a></h2>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/blog/how-to-train">How to train a new language model from scratch using Transformers and Tokenizers</a>. Blog post showing the steps to load in Esperanto data and train a
masked language model from scratch.</p></li>
<li><p><a class="reference internal" href="preprocessing.html"><span class="doc">Preprocessing</span></a>. Docs page on data preprocessing.</p></li>
<li><p><a class="reference internal" href="training.html"><span class="doc">Training</span></a>. Docs page on training and fine-tuning.</p></li>
</ul>
</div></blockquote>
<div class="section" id="using-the-nlp-datasets-metrics-library">
<span id="nlplib"></span><h3>Using the ü§ó NLP Datasets &amp; Metrics library<a class="headerlink" href="#using-the-nlp-datasets-metrics-library" title="Permalink to this headline">¬∂</a></h3>
<p>This tutorial demonstrates how to read in datasets from various raw text formats and prepare them for training with ü§ó
Transformers so that you can do the same thing with your own custom datasets. However, we recommend users use the <a class="reference external" href="https://github.com/huggingface/nlp">ü§ó
NLP library</a> for working with the 150+ datasets included in the <a class="reference external" href="https://huggingface.co/datasets">hub</a>, including the three datasets used in this tutorial. As a very brief overview, we
will show how to use the NLP library to download and prepare the IMDb dataset from the first example, <a class="reference internal" href="#seq-imdb"><span class="std std-ref">Sequence Classification with IMDb Reviews</span></a>.</p>
<p>Start by downloading the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nlp</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Each dataset has multiple columns corresponding to different features. Let‚Äôs see what our columns are.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>
<span class="go">[&#39;label&#39;, &#39;text&#39;]</span>
</pre></div>
</div>
<p>Great. Now let‚Äôs tokenize the text. We can do this using the <code class="docutils literal notranslate"><span class="pre">map</span></code> method. We‚Äôll also rename the <code class="docutils literal notranslate"><span class="pre">label</span></code> column to
<code class="docutils literal notranslate"><span class="pre">labels</span></code> to match the model‚Äôs input arguments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">batch</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train</span><span class="o">.</span><span class="n">rename_column_</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Lastly, we can use the <code class="docutils literal notranslate"><span class="pre">set_format</span></code> method to determine which columns and in what data format we want to access
dataset elements.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">train</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
<span class="p">{</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span> <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">512</span><span class="p">]),</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">512</span><span class="p">])}</span>
<span class="c1">## TENSORFLOW CODE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">train</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
<span class="p">{</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">TensorShape</span><span class="p">([]),</span> <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">TensorShape</span><span class="p">([</span><span class="mi">512</span><span class="p">]),</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">TensorShape</span><span class="p">([</span><span class="mi">512</span><span class="p">])}</span>
</pre></div>
</div>
<p>We now have a fully-prepared dataset. Check out <a class="reference external" href="https://huggingface.co/nlp/processing.html">the ü§ó NLP docs</a> for a
more thorough introduction.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel¬Æ Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>