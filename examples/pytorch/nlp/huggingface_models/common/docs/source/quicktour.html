<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick tour &mdash; Intel¬Æ Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> Intel¬Æ Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">Intel¬Æ Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel¬Æ Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel¬Æ Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Quick tour</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/quicktour.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="quick-tour">
<h1>Quick tour<a class="headerlink" href="#quick-tour" title="Permalink to this headline">¬∂</a></h1>
<p>Let‚Äôs have a quick look at the ü§ó Transformers library features. The library downloads pretrained models for Natural
Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG),
such as completing a prompt with new text or translating in another language.</p>
<p>First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we
will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All code examples presented in the documentation have a switch on the top left for Pytorch versus TensorFlow. If
not, the code is expected to work for both backends without any change needed.</p>
</div>
<div class="section" id="getting-started-on-a-task-with-a-pipeline">
<h2>Getting started on a task with a pipeline<a class="headerlink" href="#getting-started-on-a-task-with-a-pipeline" title="Permalink to this headline">¬∂</a></h2>
<p>The easiest way to use a pretrained model on a given task is to use <code class="xref py py-func docutils literal notranslate"><span class="pre">pipeline()</span></code>. ü§ó Transformers
provides the following tasks out of the box:</p>
<ul class="simple">
<li><p>Sentiment analysis: is a text positive or negative?</p></li>
<li><p>Text generation (in English): provide a prompt and the model will generate what follows.</p></li>
<li><p>Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place,
etc.)</p></li>
<li><p>Question answering: provide the model with some context and a question, extract the answer from the context.</p></li>
<li><p>Filling masked text: given a text with masked words (e.g., replaced by <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>), fill the blanks.</p></li>
<li><p>Summarization: generate a summary of a long text.</p></li>
<li><p>Translation: translate a text in another language.</p></li>
<li><p>Feature extraction: return a tensor representation of the text.</p></li>
</ul>
<p>Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the <span class="xref std std-doc">task summary</span>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will
look at both later on, but as an introduction the tokenizer‚Äôs job is to preprocess the text for the model, which is
then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to
make them readable. For instance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="p">(</span><span class="s1">&#39;We are very happy to show you the ü§ó Transformers library.&#39;</span><span class="p">)</span>
<span class="go">[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9997795224189758}]</span>
</pre></div>
</div>
<p>That‚Äôs encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model as a
<cite>batch</cite>, returning a list of dictionaries like this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">([</span><span class="s2">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span><span class="p">,</span>
<span class="gp">... </span>           <span class="s2">&quot;We hope you don&#39;t hate it.&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;label: </span><span class="si">{result[&#39;label&#39;]}</span><span class="s2">, with score: {round(result[&#39;score&#39;], 4)}&quot;</span><span class="p">)</span>
<span class="go">label: POSITIVE, with score: 0.9998</span>
<span class="go">label: NEGATIVE, with score: 0.5309</span>
</pre></div>
</div>
<p>You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is
fairly neutral.</p>
<p>By default, the model downloaded for this pipeline is called ‚Äúdistilbert-base-uncased-finetuned-sst-2-english‚Äù. We can
look at its <a class="reference external" href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">model page</a> to get more
information about it. It uses the <span class="xref std std-doc">DistilBERT architecture</span> and has been fine-tuned on a
dataset called SST-2 for the sentiment analysis task.</p>
<p>Let‚Äôs say we want to use another model; for instance, one that has been trained on French data. We can search through
the <a class="reference external" href="https://huggingface.co/models">model hub</a> that gathers models pretrained on a lot of data by research labs, but
also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags
‚ÄúFrench‚Äù and ‚Äútext-classification‚Äù gives back a suggestion ‚Äúnlptown/bert-base-multilingual-uncased-sentiment‚Äù. Let‚Äôs
see how we can use it.</p>
<p>You can directly pass the name of the model to use to <code class="xref py py-func docutils literal notranslate"><span class="pre">pipeline()</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also
replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model
object and its associated tokenizer.</p>
<p>We will need two classes for this. The first is <code class="xref py py-class docutils literal notranslate"><span class="pre">AutoTokenizer</span></code>, which we will use to download the
tokenizer associated to the model we picked and instantiate it. The second is
<code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code> (or
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFAutoModelForSequenceClassification</span></code> if you are using TensorFlow), which we will use to download
the model itself. Note that if we were using the library on an other task, the class of the model would change. The
<span class="xref std std-doc">task summary</span> tutorial summarizes which class is used for which task.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TFAutoModelForSequenceClassification</span>
</pre></div>
</div>
<p>Now, to download the models and tokenizer we found previously, we just have to use the
<code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method (feel free to replace <code class="docutils literal notranslate"><span class="pre">model_name</span></code> by
any other model from the model hub):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
<p>If you don‚Äôt find a model that has been pretrained on some data similar to yours, you will need to fine-tune a
pretrained model on your data. We provide <span class="xref std std-doc">example scripts</span> to do so. Once you‚Äôre done, don‚Äôt forget
to share your fine-tuned model on the hub with the community, using <span class="xref std std-doc">this tutorial</span>.</p>
</div>
<div class="section" id="under-the-hood-pretrained-models">
<span id="pretrained-model"></span><h2>Under the hood: pretrained models<a class="headerlink" href="#under-the-hood-pretrained-models" title="Permalink to this headline">¬∂</a></h2>
<p>Let‚Äôs now see what happens beneath the hood when using those pipelines. As we saw, the model and tokenizer are created
using the <code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained</span></code> method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TFAutoModelForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="using-the-tokenizer">
<h3>Using the tokenizer<a class="headerlink" href="#using-the-tokenizer" title="Permalink to this headline">¬∂</a></h3>
<p>We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in
words (or part of words, punctuation symbols, etc.) usually called <cite>tokens</cite>. There are multiple rules that can govern
that process (you can learn more about them in the <a class="reference internal" href="tokenizer_summary.html"><span class="doc">tokenizer summary</span></a>), which is why we need
to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was
pretrained.</p>
<p>The second step is to convert those <cite>tokens</cite> into numbers, to be able to build a tensor out of them and feed them to
the model. To do this, the tokenizer has a <cite>vocab</cite>, which is the part we download when we instantiate it with the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained</span></code> method, since we need to use the same <cite>vocab</cite> as when the model was pretrained.</p>
<p>To apply these steps on a given text, we can just feed it to our tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This returns a dictionary string to list of ints. It contains the <a class="reference external" href="glossary.html#input-ids">ids of the tokens</a>, as
mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an
<a class="reference external" href="glossary.html#attention-mask">attention mask</a> that the model will use to have a better understanding of the
sequence:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="go">{&#39;input_ids&#39;: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</span>
</pre></div>
</div>
<p>You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a
batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept
and get tensors back. You can specify all of that to the tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s2">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span><span class="p">,</span> <span class="s2">&quot;We hope you don&#39;t hate it.&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s2">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span><span class="p">,</span> <span class="s2">&quot;We hope you don&#39;t hate it.&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;tf&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding
token the model was pretrained with. The attention mask is also adapted to take the padding into account:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">pt_batch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;</span><span class="si">{key}</span><span class="s2">: {value.numpy().tolist()}&quot;</span><span class="p">)</span>
<span class="go">input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]</span>
<span class="go">attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">tf_batch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;</span><span class="si">{key}</span><span class="s2">: {value.numpy().tolist()}&quot;</span><span class="p">)</span>
<span class="go">input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]</span>
<span class="go">attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]</span>
</pre></div>
</div>
<p>You can learn more about tokenizers <a class="reference internal" href="preprocessing.html"><span class="doc">here</span></a>.</p>
</div>
<div class="section" id="using-the-model">
<h3>Using the model<a class="headerlink" href="#using-the-model" title="Permalink to this headline">¬∂</a></h3>
<p>Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will
contain all the relevant information the model needs. If you‚Äôre using a TensorFlow model, you can pass the dictionary
keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding <code class="xref py py-obj docutils literal notranslate"><span class="pre">**</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_outputs</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">(</span><span class="o">**</span><span class="n">pt_batch</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_outputs</span> <span class="o">=</span> <span class="n">tf_model</span><span class="p">(</span><span class="n">tf_batch</span><span class="p">)</span>
</pre></div>
</div>
<p>In ü§ó Transformers, all outputs are tuples (with only one element potentially). Here, we get a tuple with just the final
activations of the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pt_outputs</span><span class="p">)</span>
<span class="go">(tensor([[-4.0833,  4.3364],</span>
<span class="go">        [ 0.0818, -0.0418]], grad_fn=&lt;AddmmBackward&gt;),)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf_outputs</span><span class="p">)</span>
<span class="go">(&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=</span>
<span class="go">array([[-4.0832963 ,  4.336414  ],</span>
<span class="go">       [ 0.08181786, -0.04179301]], dtype=float32)&gt;,)</span>
</pre></div>
</div>
<p>The model can return more than just the final activations, which is why the output is a tuple. Here we only asked for
the final activations, so we get a tuple with one element.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All ü§ó Transformers models (PyTorch or TensorFlow) return the activations of the model <em>before</em> the final activation
function (like SoftMax) since this final activation function is often fused with the loss.</p>
</div>
<p>Let‚Äôs apply the SoftMax activation to get predictions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_predictions</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pt_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>We can see we get the numbers from before:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tf_predictions</span><span class="p">)</span>
<span class="go">tf.Tensor(</span>
<span class="go">[[2.2042994e-04 9.9977952e-01]</span>
<span class="go"> [5.3086340e-01 4.6913657e-01]], shape=(2, 2), dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pt_predictions</span><span class="p">)</span>
<span class="go">tensor([[2.2043e-04, 9.9978e-01],</span>
<span class="go">        [5.3086e-01, 4.6914e-01]], grad_fn=&lt;SoftmaxBackward&gt;)</span>
</pre></div>
</div>
<p>If you have labels, you can provide them to the model, it will return a tuple with the loss and the final activations.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_outputs</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">(</span><span class="o">**</span><span class="n">pt_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_outputs</span> <span class="o">=</span> <span class="n">tf_model</span><span class="p">(</span><span class="n">tf_batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<p>Models are standard <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> or <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a> so you can use them in your usual training loop. ü§ó
Transformers also provides a <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> (or <code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code> if you are using
TensorFlow) class to help with your training (taking care of things such as distributed training, mixed precision,
etc.). See the <a class="reference internal" href="training.html"><span class="doc">training tutorial</span></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE.
They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which
case the attributes not set (that have <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> values) are ignored.</p>
</div>
<p>Once your model is fine-tuned, you can save it with its tokenizer in the following way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
</pre></div>
</div>
<p>You can then load this model back using the <code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method by passing the
directory name instead of the model name. One cool feature of ü§ó Transformers is that you can easily switch between
PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow. If you are
loading a saved PyTorch model in a TensorFlow model, use <code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>and if you are loading a saved TensorFlow model in a PyTorch model, you should use the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Lastly, you can also ask the model to return all hidden states and all attention weights if you need them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_outputs</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">(</span><span class="o">**</span><span class="n">pt_batch</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">pt_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_outputs</span> <span class="o">=</span> <span class="n">tf_model</span><span class="p">(</span><span class="n">tf_batch</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_attentions</span> <span class="o">=</span> <span class="n">tf_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-the-code">
<h3>Accessing the code<a class="headerlink" href="#accessing-the-code" title="Permalink to this headline">¬∂</a></h3>
<p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">AutoModel</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> classes are just shortcuts that will automatically work with any
pretrained model. Behind the scenes, the library has one model class per combination of architecture plus class, so the
code is easy to access and tweak if you need to.</p>
<p>In our previous example, the model was called ‚Äúdistilbert-base-uncased-finetuned-sst-2-english‚Äù, which means it‚Äôs using
the <span class="xref std std-doc">DistilBERT</span> architecture. As
<code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code> (or
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFAutoModelForSequenceClassification</span></code> if you are using TensorFlow) was used, the model
automatically created is then a <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code>. You can look at its
documentation for all details relevant to that specific model, or browse the source code. This is how you would
directly instantiate model and tokenizer without the auto magic:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="customizing-the-model">
<h3>Customizing the model<a class="headerlink" href="#customizing-the-model" title="Permalink to this headline">¬∂</a></h3>
<p>If you want to change how the model itself is built, you can define your custom configuration class. Each architecture
comes with its own relevant configuration (in the case of DistilBERT, <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code>) which
allows you to specify any of the hidden dimension, dropout rate, etc. If you do core modifications, like changing the
hidden size, you won‚Äôt be able to use a pretrained model anymore and will need to train from scratch. You would then
instantiate the model directly from this configuration.</p>
<p>Here we use the predefined vocabulary of DistilBERT (hence load the tokenizer with the
<code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method) and initialize the model from scratch (hence
instantiate the model from the configuration instead of using the
<code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertConfig</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">(</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="mi">512</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertConfig</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">(</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="mi">512</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>For something that only changes the head of the model (for instance, the number of labels), you can still use a
pretrained model for the body. For instance, let‚Äôs define a classifier for 10 different labels using a pretrained body.
We could create a configuration with all the default values and just change the number of labels, but more easily, you
can directly pass any argument a configuration would take to the <code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method and it will update the
default configuration with it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertConfig</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">DistilBertConfig</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">TFDistilBertForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel¬Æ Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>