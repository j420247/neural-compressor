<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Perplexity of fixed-length models &mdash; IntelÂ® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> IntelÂ® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">IntelÂ® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">IntelÂ® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">IntelÂ® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Perplexity of fixed-length models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/docs/source/perplexity.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="perplexity-of-fixed-length-models">
<h1>Perplexity of fixed-length models<a class="headerlink" href="#perplexity-of-fixed-length-models" title="Permalink to this headline">Â¶</a></h1>
<p>Perplexity (PPL) is one of the most common metrics for evaluating language models. Before diving in, we should note
that the metric applies specifically to classical language models (sometimes called autoregressive or causal language
models) and is not well defined for masked language models like BERT (see <a class="reference internal" href="model_summary.html"><span class="doc">summary of the models</span></a>).</p>
<p>Perplexity is defined as the exponentiated average log-likelihood of a sequence. If we have a tokenized sequence
<span class="math notranslate nohighlight">\(X = (x_0, x_1, \dots, x_t)\)</span>, then the perplexity of <span class="math notranslate nohighlight">\(X\)</span> is,</p>
<div class="math notranslate nohighlight">
\[\text{PPL}(X)
= \exp \left\{ {-\frac{1}{t}\sum_i^t \log p_\theta (x_i|x_{&lt;i}) } \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\log p_\theta (x_i|x_{&lt;i})\)</span> is the log-likelihood of the ith token conditioned on the preceding tokens
<span class="math notranslate nohighlight">\(x_{&lt;i}\)</span> according to our model. Intuitively, it can be thought of as an evaluation of the modelâ€™s ability to
predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization
procedure has a direct impact on a modelâ€™s perplexity which should always be taken into consideration when comparing
different models.</p>
<p>This is also equivalent to the exponentiation of the cross-entropy between the data and model predictions. For more
intuition about perplexity and its relationship to Bits Per Character (BPC) and data compression, check out this
<a class="reference external" href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">fantastic blog post on The Gradient</a>.</p>
<div class="section" id="calculating-ppl-with-fixed-length-models">
<h2>Calculating PPL with fixed-length models<a class="headerlink" href="#calculating-ppl-with-fixed-length-models" title="Permalink to this headline">Â¶</a></h2>
<p>If we werenâ€™t limited by a modelâ€™s context size, we would evaluate the modelâ€™s perplexity by autoregressively
factorizing a sequence and conditioning on the entire preceding subsequence at each step, as shown below.</p>
<a class="reference internal image-reference" href="../../../../../../../_images/ppl_full.gif"><img alt="Full decomposition of a sequence with unlimited context length" src="../../../../../../../_images/ppl_full.gif" style="width: 600px;" /></a>
<p>When working with approximate models, however, we typically have a constraint on the number of tokens the model can
process. The largest version of <a class="reference internal" href="model_doc/gpt2.html"><span class="doc">GPT-2</span></a>, for example, has a fixed length of 1024 tokens, so we
cannot calculate <span class="math notranslate nohighlight">\(p_\theta(x_t|x_{&lt;t})\)</span> directly when <span class="math notranslate nohighlight">\(t\)</span> is greater than 1024.</p>
<p>Instead, the sequence is typically broken into subsequences equal to the modelâ€™s maximum input size. If a modelâ€™s max
input size is <span class="math notranslate nohighlight">\(k\)</span>, we then approximate the likelihood of a token <span class="math notranslate nohighlight">\(x_t\)</span> by conditioning only on the
<span class="math notranslate nohighlight">\(k-1\)</span> tokens that precede it rather than the entire context. When evaluating the modelâ€™s perplexity of a
sequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed
log-likelihoods of each segment independently.</p>
<a class="reference internal image-reference" href="../../../../../../../_images/ppl_chunked.gif"><img alt="Suboptimal PPL not taking advantage of full available context" src="../../../../../../../_images/ppl_chunked.gif" style="width: 600px;" /></a>
<p>This is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor
approximation of the fully-factorized perplexity and will typically yield a higher (worse) PPL because the model will
have less context at most of the prediction steps.</p>
<p>Instead, the PPL of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly
sliding the context window so that the model has more context when making each prediction.</p>
<a class="reference internal image-reference" href="../../../../../../../_images/ppl_sliding.gif"><img alt="Sliding window PPL taking advantage of all available context" src="../../../../../../../_images/ppl_sliding.gif" style="width: 600px;" /></a>
<p>This is a closer approximation to the true decomposition of the sequence probability and will typically yield a more
favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good
practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by
1 token a time. This allows computation to proceed much faster while still giving the model a large context to make
predictions at each step.</p>
</div>
<div class="section" id="example-calculating-perplexity-with-gpt-2-in-transformers">
<h2>Example: Calculating perplexity with GPT-2 in ðŸ¤— Transformers<a class="headerlink" href="#example-calculating-perplexity-with-gpt-2-in-transformers" title="Permalink to this headline">Â¶</a></h2>
<p>Letâ€™s demonstrate this process with GPT-2.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2TokenizerFast</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s1">&#39;gpt2-large&#39;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Weâ€™ll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-window strategies. Since
this dataset is small and weâ€™re just doing one forward pass over the set, we can just load and encode the entire
dataset in memory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nlp</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;wikitext&#39;</span><span class="p">,</span> <span class="s1">&#39;wikitext-2-raw-v1&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
<span class="n">encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>With ðŸ¤— Transformers, we can simply pass the <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> as the <code class="docutils literal notranslate"><span class="pre">labels</span></code> to our model, and the average
log-likelihood for each token is returned as the loss. With our sliding window approach, however, there is overlap in
the tokens we pass to the model at each iteration. We donâ€™t want the log-likelihood for the tokens weâ€™re just treating
as context to be included in our loss, so we can set these targets to <code class="docutils literal notranslate"><span class="pre">-100</span></code> so that they are ignored. The following
is an example of how we could do this with a stride of <code class="docutils literal notranslate"><span class="pre">512</span></code>. This means that the model will have at least 512 tokens
for context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens
available to condition on).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_positions</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">512</span>

<span class="n">lls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">encodings</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="p">)):</span>
    <span class="n">begin_loc</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end_loc</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">stride</span><span class="p">,</span> <span class="n">encodings</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">trg_len</span> <span class="o">=</span> <span class="n">end_loc</span> <span class="o">-</span> <span class="n">i</span>    <span class="c1"># may be different from stride on last loop</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encodings</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[:,</span><span class="n">begin_loc</span><span class="p">:</span><span class="n">end_loc</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">target_ids</span><span class="p">[:,:</span><span class="o">-</span><span class="n">trg_len</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">target_ids</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">trg_len</span>

    <span class="n">lls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">)</span>

<span class="n">ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">end_loc</span><span class="p">)</span>
</pre></div>
</div>
<p>Running this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window
strategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,
and the better the reported perplexity will typically be.</p>
<p>When we run the above with <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">=</span> <span class="pre">1024</span></code>, i.e. no overlap, the resulting PPL is <code class="docutils literal notranslate"><span class="pre">19.64</span></code>, which is about the same
as the <code class="docutils literal notranslate"><span class="pre">19.93</span></code> reported in the GPT-2 paper. By using <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">=</span> <span class="pre">512</span></code> and thereby employing our striding window
strategy, this jumps down to <code class="docutils literal notranslate"><span class="pre">16.53</span></code>. This is not only a more favorable score, but is calculated in a way that is
closer to the true autoregressive decomposition of a sequence likelihood.</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IntelÂ® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>