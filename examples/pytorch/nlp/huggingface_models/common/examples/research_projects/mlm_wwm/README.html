<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Whole Word Mask Language Model &mdash; IntelÂ® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../../" src="../../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../../index.html" class="icon icon-home"> IntelÂ® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../README.html">IntelÂ® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">IntelÂ® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">IntelÂ® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Whole Word Mask Language Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/examples/research_projects/mlm_wwm/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="whole-word-mask-language-model">
<h1>Whole Word Mask Language Model<a class="headerlink" href="#whole-word-mask-language-model" title="Permalink to this headline">Â¶</a></h1>
<p>These scripts leverage the ðŸ¤— Datasets library and the Trainer API. You can easily customize them to your needs if you
need extra processing on your datasets.</p>
<p>The following examples, will run on a datasets hosted on our <a class="reference external" href="https://huggingface.co/datasets">hub</a> or with your own
text files for training and validation. We give examples of both below.</p>
<p>The BERT authors released a new version of BERT using Whole Word Masking in May 2019. Instead of masking randomly
selected tokens (which may be part of words), they mask randomly selected words (masking all the tokens corresponding
to that word). This technique has been refined for Chinese in <a class="reference external" href="https://arxiv.org/abs/1906.08101">this paper</a>.</p>
<p>To fine-tune a model using whole word masking, use the following script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_mlm_wwm.py <span class="se">\</span>
    --model_name_or_path roberta-base <span class="se">\</span>
    --dataset_name wikitext <span class="se">\</span>
    --dataset_config_name wikitext-2-raw-v1 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-mlm-wwm
</pre></div>
</div>
<p>For Chinese models, we need to generate a reference files (which requires the ltp library), because itâ€™s tokenized at
the character level.</p>
<p><strong>Q :</strong> Why a reference file?</p>
<p><strong>A :</strong> Suppose we have a Chinese sentence like: <code class="docutils literal notranslate"><span class="pre">æˆ‘å–œæ¬¢ä½ </span></code> The original Chinese-BERT will tokenize it as
<code class="docutils literal notranslate"><span class="pre">['æˆ‘','å–œ','æ¬¢','ä½ ']</span></code> (character level). But <code class="docutils literal notranslate"><span class="pre">å–œæ¬¢</span></code> is a whole word. For whole word masking proxy, we need a result
like <code class="docutils literal notranslate"><span class="pre">['æˆ‘','å–œ','##æ¬¢','ä½ ']</span></code>, so we need a reference file to tell the model which position of the BERT original token
should be added <code class="docutils literal notranslate"><span class="pre">##</span></code>.</p>
<p><strong>Q :</strong> Why LTP ?</p>
<p><strong>A :</strong> Cause the best known Chinese WWM BERT is <a class="reference external" href="https://github.com/ymcui/Chinese-BERT-wwm">Chinese-BERT-wwm</a> by HIT.
It works well on so many Chines Task like CLUE (Chinese GLUE). They use LTP, so if we want to fine-tune their model,
we need LTP.</p>
<p>You could run the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">LTP_RESOURCE</span><span class="o">=</span>/path/to/ltp/tokenizer
<span class="nb">export</span> <span class="nv">BERT_RESOURCE</span><span class="o">=</span>/path/to/bert/tokenizer
<span class="nb">export</span> <span class="nv">SAVE_PATH</span><span class="o">=</span>/path/to/data/ref.txt

python run_chinese_ref.py <span class="se">\</span>
    --file_name<span class="o">=</span>path_to_train_or_eval_file <span class="se">\</span>
    --ltp<span class="o">=</span>path_to_ltp_tokenizer <span class="se">\</span>
    --bert<span class="o">=</span>path_to_bert_tokenizer <span class="se">\</span>
    --save_path<span class="o">=</span>path_to_reference_file
</pre></div>
</div>
<p>Then you can run the script like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_mlm_wwm.py <span class="se">\</span>
    --model_name_or_path roberta-base <span class="se">\</span>
    --train_file path_to_train_file <span class="se">\</span>
    --validation_file path_to_validation_file <span class="se">\</span>
    --train_ref_file path_to_train_chinese_ref_file <span class="se">\</span>
    --validation_ref_file path_to_validation_chinese_ref_file <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-mlm-wwm
</pre></div>
</div>
<p><strong>Note1:</strong> On TPU, you should the flag <code class="docutils literal notranslate"><span class="pre">--pad_to_max_length</span></code> to make sure all your batches have the same length.</p>
<p><strong>Note2:</strong> And if you have any questions or something goes wrong when runing this code, donâ€™t hesitate to pin &#64;wlhgtc.</p>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IntelÂ® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>