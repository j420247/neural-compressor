<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sequence to Sequence Training and Evaluation &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../../" src="../../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Sequence to Sequence Training and Evaluation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../_sources/examples/pytorch/nlp/huggingface_models/common/examples/research_projects/seq2seq-distillation/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="sequence-to-sequence-training-and-evaluation">
<h1>Sequence to Sequence Training and Evaluation<a class="headerlink" href="#sequence-to-sequence-training-and-evaluation" title="Permalink to this headline">¶</a></h1>
<p>This directory contains examples for finetuning and evaluating transformers on summarization and translation tasks.</p>
<p>Author: Sam Shleifer (https://github.com/sshleifer)</p>
<div class="section" id="supported-architectures">
<h2>Supported Architectures<a class="headerlink" href="#supported-architectures" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code> (and anything that inherits from it)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MarianMTModel</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PegasusForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MBartForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="xsum">
<h2>XSUM<a class="headerlink" href="#xsum" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/contrib/pytorch-lightning/seq2seq
wget https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz
tar -xzvf xsum.tar.gz
<span class="nb">export</span> <span class="nv">XSUM_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/xsum
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">xsum/</span></code> with files like <code class="docutils literal notranslate"><span class="pre">test.source</span></code>.
To use your own data, copy that files format. Each article to be summarized is on its own line.</p>
</div>
<div class="section" id="cnn-dailymail">
<h2>CNN/DailyMail<a class="headerlink" href="#cnn-dailymail" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/contrib/pytorch-lightning/seq2seq
wget https://cdn-datasets.huggingface.co/summarization/cnn_dm_v2.tgz
tar -xzvf cnn_dm_v2.tgz  <span class="c1"># empty lines removed</span>
mv cnn_cln cnn_dm
<span class="nb">export</span> <span class="nv">CNN_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/cnn_dm
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">cnn_dm/</span></code> with 6 files.</p>
</div>
<div class="section" id="wmt16-english-romanian-translation-data">
<h2>WMT16 English-Romanian Translation Data<a class="headerlink" href="#wmt16-english-romanian-translation-data" title="Permalink to this headline">¶</a></h2>
<p>download with this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://cdn-datasets.huggingface.co/translation/wmt_en_ro.tar.gz
tar -xzvf wmt_en_ro.tar.gz
<span class="nb">export</span> <span class="nv">ENRO_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/wmt_en_ro
</pre></div>
</div>
<p>this should make a directory called <code class="docutils literal notranslate"><span class="pre">wmt_en_ro/</span></code> with 6 files.</p>
</div>
<div class="section" id="wmt-english-german">
<h2>WMT English-German<a class="headerlink" href="#wmt-english-german" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://cdn-datasets.huggingface.co/translation/wmt_en_de.tgz
tar -xzvf wmt_en_de.tgz
<span class="nb">export</span> <span class="nv">DATA_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">PWD</span><span class="si">}</span>/wmt_en_de
</pre></div>
</div>
</div>
<div class="section" id="fsmt-datasets-wmt">
<h2>FSMT datasets (wmt)<a class="headerlink" href="#fsmt-datasets-wmt" title="Permalink to this headline">¶</a></h2>
<p>Refer to the scripts starting with <code class="docutils literal notranslate"><span class="pre">eval_</span></code> under:
https://github.com/huggingface/transformers/tree/master/scripts/fsmt</p>
</div>
<div class="section" id="pegasus-multiple-datasets">
<h2>Pegasus (multiple datasets)<a class="headerlink" href="#pegasus-multiple-datasets" title="Permalink to this headline">¶</a></h2>
<p>Multiple eval datasets are available for download from:
https://github.com/stas00/porting/tree/master/datasets/pegasus</p>
</div>
<div class="section" id="your-data">
<h2>Your Data<a class="headerlink" href="#your-data" title="Permalink to this headline">¶</a></h2>
<p>If you are using your own data, it must be formatted as one directory with 6 files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">source</span>
<span class="n">train</span><span class="o">.</span><span class="n">target</span>
<span class="n">val</span><span class="o">.</span><span class="n">source</span>
<span class="n">val</span><span class="o">.</span><span class="n">target</span>
<span class="n">test</span><span class="o">.</span><span class="n">source</span>
<span class="n">test</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.source</span></code> files are the input, the <code class="docutils literal notranslate"><span class="pre">.target</span></code> files are the desired output.</p>
</div>
<div class="section" id="potential-issues">
<h2>Potential issues<a class="headerlink" href="#potential-issues" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>native AMP (<code class="docutils literal notranslate"><span class="pre">--fp16</span></code> and no apex) may lead to a huge memory leak and require 10x gpu memory. This has been fixed in pytorch-nightly and the minimal official version to have this fix will be pytorch-1.8. Until then if you have to use mixed precision please use AMP only with pytorch-nightly or NVIDIA’s apex. Reference: https://github.com/huggingface/transformers/issues/8403</p></li>
</ul>
</div>
<div class="section" id="tips-and-tricks">
<h2>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this headline">¶</a></h2>
<p>General Tips:</p>
<ul>
<li><p>since you need to run from this folder, and likely need to modify code, the easiest workflow is fork transformers, clone your fork, and run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code> before you get started.</p></li>
<li><p>try <code class="docutils literal notranslate"><span class="pre">--freeze_encoder</span></code> or <code class="docutils literal notranslate"><span class="pre">--freeze_embeds</span></code> for faster training/larger batch size.  (3hr per epoch with bs=8, see the “xsum_shared_task” command below)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fp16_opt_level=O1</span></code> (the default works best).</p></li>
<li><p>In addition to the pytorch-lightning .ckpt checkpoint, a transformers checkpoint will be saved.
Load it with <code class="docutils literal notranslate"><span class="pre">BartForConditionalGeneration.from_pretrained(f'{output_dir}/best_tfmr)</span></code>.</p></li>
<li><p>At the moment, <code class="docutils literal notranslate"><span class="pre">--do_predict</span></code> does not work in a multi-gpu setting. You need to use <code class="docutils literal notranslate"><span class="pre">evaluate_checkpoint</span></code> or the <code class="docutils literal notranslate"><span class="pre">run_eval.py</span></code> code.</p></li>
<li><p>This warning can be safely ignored:</p>
<blockquote>
<div><p>“Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: [‘final_logits_bias’]”</p>
</div></blockquote>
</li>
<li><p>Both finetuning and eval are 30% faster with <code class="docutils literal notranslate"><span class="pre">--fp16</span></code>. For that you need to <a class="reference external" href="https://github.com/NVIDIA/apex#quick-start">install apex</a>.</p></li>
<li><p>Read scripts before you run them!</p></li>
</ul>
<p>Summarization Tips:</p>
<ul class="simple">
<li><p>(summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB GPU RAM with fp16 on an NVIDIA-V100.</p></li>
<li><p>If you want to run experiments on improving the summarization finetuning process, try the XSUM Shared Task (below). It’s faster to train than CNNDM because the summaries are shorter.</p></li>
<li><p>For CNN/DailyMail, the default <code class="docutils literal notranslate"><span class="pre">val_max_target_length</span></code> and <code class="docutils literal notranslate"><span class="pre">test_max_target_length</span></code> will truncate the ground truth labels, resulting in slightly higher rouge scores. To get accurate rouge scores, you should rerun calculate_rouge on the <code class="docutils literal notranslate"><span class="pre">{output_dir}/test_generations.txt</span></code> file saved by <code class="docutils literal notranslate"><span class="pre">trainer.test()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--max_target_length=60</span> <span class="pre">--val_max_target_length=60</span> <span class="pre">--test_max_target_length=100</span> </code> is a reasonable setting for XSUM.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wandb</span></code> can be used by specifying <code class="docutils literal notranslate"><span class="pre">--logger_name</span> <span class="pre">wandb</span></code>. It is useful for reproducibility. Specify the environment variable <code class="docutils literal notranslate"><span class="pre">WANDB_PROJECT='hf_xsum'</span></code> to do the XSUM shared task.</p></li>
<li><p>If you are finetuning on your own dataset, start from <code class="docutils literal notranslate"><span class="pre">distilbart-cnn-12-6</span></code> if you want long summaries and <code class="docutils literal notranslate"><span class="pre">distilbart-xsum-12-6</span></code> if you want short summaries.
(It rarely makes sense to start from <code class="docutils literal notranslate"><span class="pre">bart-large</span></code> unless you are a researching finetuning methods).</p></li>
</ul>
<p><strong>Update 2018-07-18</strong>
Datasets: <code class="docutils literal notranslate"><span class="pre">LegacySeq2SeqDataset</span></code> will be used for all tokenizers without a <code class="docutils literal notranslate"><span class="pre">prepare_seq2seq_batch</span></code> method. Otherwise, <code class="docutils literal notranslate"><span class="pre">Seq2SeqDataset</span></code> will be used.
Future work/help wanted: A new dataset to support multilingual tasks.</p>
</div>
<div class="section" id="finetuning-scripts">
<h2>Finetuning Scripts<a class="headerlink" href="#finetuning-scripts" title="Permalink to this headline">¶</a></h2>
<p>All finetuning bash scripts call finetune.py (or distillation.py) with reasonable command line arguments. They usually require extra command line arguments to work.</p>
<p>To see all the possible command line options, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./finetune.py --help
</pre></div>
</div>
</div>
<div class="section" id="finetuning-training-params">
<h2>Finetuning Training Params<a class="headerlink" href="#finetuning-training-params" title="Permalink to this headline">¶</a></h2>
<p>To override the pretrained model’s training params, you can pass them to <code class="docutils literal notranslate"><span class="pre">./finetune.sh</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./finetune.sh <span class="se">\</span>
    <span class="o">[</span>...<span class="o">]</span>
    --encoder_layerdrop <span class="m">0</span>.1 <span class="se">\</span>
    --decoder_layerdrop <span class="m">0</span>.1 <span class="se">\</span>
    --dropout <span class="m">0</span>.1 <span class="se">\</span>
    --attention_dropout <span class="m">0</span>.1 <span class="se">\</span>
</pre></div>
</div>
</div>
<div class="section" id="summarization-finetuning">
<h2>Summarization Finetuning<a class="headerlink" href="#summarization-finetuning" title="Permalink to this headline">¶</a></h2>
<p>Run/modify <code class="docutils literal notranslate"><span class="pre">finetune.sh</span></code></p>
<p>The following command should work on a 16GB GPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./finetune.sh <span class="se">\</span>
    --data_dir <span class="nv">$XSUM_DIR</span> <span class="se">\</span>
    --train_batch_size<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
    --eval_batch_size<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
    --output_dir<span class="o">=</span>xsum_results <span class="se">\</span>
    --num_train_epochs <span class="m">6</span> <span class="se">\</span>
    --model_name_or_path facebook/bart-large
</pre></div>
</div>
<p>There is a starter finetuning script for pegasus at <code class="docutils literal notranslate"><span class="pre">finetune_pegasus_xsum.sh</span></code>.</p>
</div>
<div class="section" id="translation-finetuning">
<h2>Translation Finetuning<a class="headerlink" href="#translation-finetuning" title="Permalink to this headline">¶</a></h2>
<p>First, follow the wmt_en_ro download instructions.
Then you can finetune mbart_cc25 on english-romanian with the following command.
<strong>Recommendation:</strong> Read and potentially modify the fairly opinionated defaults in <code class="docutils literal notranslate"><span class="pre">train_mbart_cc25_enro.sh</span></code> script before running it.</p>
<p>Best performing command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># optionally</span>
<span class="nb">export</span> <span class="nv">ENRO_DIR</span><span class="o">=</span><span class="s1">&#39;wmt_en_ro&#39;</span> <span class="c1"># Download instructions above</span>
<span class="c1"># export WANDB_PROJECT=&quot;MT&quot; # optional</span>
<span class="nb">export</span> <span class="nv">MAX_LEN</span><span class="o">=</span><span class="m">128</span>
<span class="nb">export</span> <span class="nv">BS</span><span class="o">=</span><span class="m">4</span>
./train_mbart_cc25_enro.sh --output_dir enro_finetune_baseline --label_smoothing <span class="m">0</span>.1 --fp16_opt_level<span class="o">=</span>O1 --logger_name wandb --sortish_sampler
</pre></div>
</div>
<p>This should take &lt; 6h/epoch on a 16GB v100 and achieve test BLEU above 26
To get results in line with fairseq, you need to do some postprocessing. (see <code class="docutils literal notranslate"><span class="pre">romanian_postprocessing.md</span></code>)</p>
<p>MultiGPU command
(using 8 GPUS as an example)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">ENRO_DIR</span><span class="o">=</span><span class="s1">&#39;wmt_en_ro&#39;</span> <span class="c1"># Download instructions above</span>
 <span class="c1"># export WANDB_PROJECT=&quot;MT&quot; # optional</span>
<span class="nb">export</span> <span class="nv">MAX_LEN</span><span class="o">=</span><span class="m">128</span>
<span class="nb">export</span> <span class="nv">BS</span><span class="o">=</span><span class="m">4</span>
./train_mbart_cc25_enro.sh --output_dir enro_finetune_baseline --gpus <span class="m">8</span> --logger_name wandb
</pre></div>
</div>
</div>
<div class="section" id="finetuning-outputs">
<h2>Finetuning Outputs<a class="headerlink" href="#finetuning-outputs" title="Permalink to this headline">¶</a></h2>
<p>As you train, <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> will be filled with files, that look kind of like this (comments are mine).
Some of them are metrics, some of them are checkpoints, some of them are metadata. Here is a quick tour:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>output_dir
├── best_tfmr  <span class="c1"># this is a huggingface checkpoint generated by save_pretrained. It is the same model as the PL .ckpt file below</span>
│   ├── config.json
│   ├── merges.txt
│   ├── pytorch_model.bin
│   ├── special_tokens_map.json
│   ├── tokenizer_config.json
│   └── vocab.json
├── git_log.json   <span class="c1"># repo, branch, and commit hash</span>
├── <span class="nv">val_avg_rouge2</span><span class="o">=</span><span class="m">0</span>.1984-step_count<span class="o">=</span><span class="m">11</span>.ckpt  <span class="c1"># this is a pytorch lightning checkpoint associated with the best val score. (it will be called BLEU for MT)</span>
├── metrics.json  <span class="c1"># new validation metrics will continually be appended to this</span>
├── student  <span class="c1"># this is a huggingface checkpoint generated by SummarizationDistiller. It is the student before it gets finetuned.</span>
│   ├── config.json
│   └── pytorch_model.bin
├── test_generations.txt
<span class="c1"># ^^ are the summaries or translations produced by your best checkpoint on the test data. Populated when training is done</span>
├── test_results.txt  <span class="c1"># a convenience file with the test set metrics. This data is also in metrics.json[&#39;test&#39;]</span>
├── hparams.pkl  <span class="c1"># the command line args passed after some light preprocessing. Should be saved fairly quickly.</span>
</pre></div>
</div>
<p>After training, you can recover the best checkpoint by running</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;{output_dir}/best_tfmr&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="converting-pytorch-lightning-checkpoints">
<h2>Converting pytorch-lightning checkpoints<a class="headerlink" href="#converting-pytorch-lightning-checkpoints" title="Permalink to this headline">¶</a></h2>
<p>pytorch lightning <code class="docutils literal notranslate"><span class="pre">-do_predict</span></code> often fails, after you are done training, the best way to evaluate your model is to convert it.</p>
<p>This should be done for you, with a file called <code class="docutils literal notranslate"><span class="pre">{save_dir}/best_tfmr</span></code>.</p>
<p>If that file doesn’t exist but you have a lightning <code class="docutils literal notranslate"><span class="pre">.ckpt</span></code> file, you can run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python convert_pl_checkpoint_to_hf.py PATH_TO_CKPT  randomly_initialized_hf_model_path save_dir/best_tfmr
</pre></div>
</div>
<p>Then either <code class="docutils literal notranslate"><span class="pre">run_eval</span></code> or <code class="docutils literal notranslate"><span class="pre">run_distributed_eval</span></code> with <code class="docutils literal notranslate"><span class="pre">save_dir/best_tfmr</span></code> (see previous sections)</p>
</div>
</div>
<div class="section" id="experimental-features">
<h1>Experimental Features<a class="headerlink" href="#experimental-features" title="Permalink to this headline">¶</a></h1>
<p>These features are harder to use and not always useful.</p>
<div class="section" id="dynamic-batch-size-for-mt">
<h2>Dynamic Batch Size for MT<a class="headerlink" href="#dynamic-batch-size-for-mt" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">finetune.py</span></code> has a command line arg <code class="docutils literal notranslate"><span class="pre">--max_tokens_per_batch</span></code> that allows batches to be dynamically sized.
This feature can only be used:</p>
<ul class="simple">
<li><p>with fairseq installed</p></li>
<li><p>on 1 GPU</p></li>
<li><p>without sortish sampler</p></li>
<li><p>after calling <code class="docutils literal notranslate"><span class="pre">./save_len_file.py</span> <span class="pre">$tok</span> <span class="pre">$data_dir</span></code></p></li>
</ul>
<p>For example,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./save_len_file.py Helsinki-NLP/opus-mt-en-ro  wmt_en_ro
./dynamic_bs_example.sh --max_tokens_per_batch<span class="o">=</span><span class="m">2000</span> --output_dir benchmark_dynamic_bs
</pre></div>
</div>
<p>splits <code class="docutils literal notranslate"><span class="pre">wmt_en_ro/train</span></code> into 11,197 uneven lengthed batches and can finish 1 epoch in 8 minutes on a v100.</p>
<p>For comparison,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./dynamic_bs_example.sh --sortish_sampler --train_batch_size <span class="m">48</span>
</pre></div>
</div>
<p>uses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.</p>
<p>The feature is still experimental, because:</p>
<ul class="simple">
<li><p>we can make it much more robust if we have memory mapped/preprocessed datasets.</p></li>
<li><p>The speedup over sortish sampler is not that large at the moment.</p></li>
</ul>
</div>
</div>
<div class="section" id="distilbart">
<h1>DistilBART<a class="headerlink" href="#distilbart" title="Permalink to this headline">¶</a></h1>
<!---It should be called distilling bart and pegasus, but I don't want to break the link in the paper.--><p>This section describes all code and artifacts from our <a class="reference external" href="http://arxiv.org/abs/2010.13002">Paper</a></p>
<p><img alt="DBART" src="https://huggingface.co/front/thumbnails/distilbart_large.png" /></p>
<ul class="simple">
<li><p>For the CNN/DailyMail dataset, (relatively longer, more extractive summaries), we found a simple technique that works, which we call “Shrink and Fine-tune”, or SFT.
you just copy alternating layers from <code class="docutils literal notranslate"><span class="pre">facebook/bart-large-cnn</span></code> and fine-tune more on the cnn/dm data. <code class="docutils literal notranslate"><span class="pre">sshleifer/distill-pegasus-cnn-16-4</span></code>, <code class="docutils literal notranslate"><span class="pre">sshleifer/distilbart-cnn-12-6</span></code> and all other checkpoints under <code class="docutils literal notranslate"><span class="pre">sshleifer</span></code> that start with <code class="docutils literal notranslate"><span class="pre">distilbart-cnn</span></code> were trained this way.</p></li>
<li><p>For the XSUM dataset, training on pseudo-labels worked best for Pegasus (<code class="docutils literal notranslate"><span class="pre">sshleifer/distill-pegasus-16-4</span></code>), while training with KD worked best for <code class="docutils literal notranslate"><span class="pre">distilbart-xsum-12-6</span></code></p></li>
<li><p>For <code class="docutils literal notranslate"><span class="pre">sshleifer/dbart-xsum-12-3</span></code></p></li>
<li><p>We ran 100s experiments, and didn’t want to document 100s of commands. If you want a command to replicate a figure from the paper that is not documented below, feel free to ask on the <a class="reference external" href="https://discuss.huggingface.co/t/seq2seq-distillation-methodology-questions/1270">forums</a> and tag <code class="docutils literal notranslate"><span class="pre">&#64;sshleifer</span></code>.</p></li>
<li><p>You can see the performance tradeoffs of model sizes <a class="reference external" href="https://docs.google.com/spreadsheets/d/1EkhDMwVO02m8jCD1cG3RoFPLicpcL1GQHTQjfvDYgIM/edit#gid=0">here</a>.
and more granular timing results <a class="reference external" href="https://docs.google.com/spreadsheets/d/1EkhDMwVO02m8jCD1cG3RoFPLicpcL1GQHTQjfvDYgIM/edit#gid=1753259047&amp;range=B2:I23">here</a>.</p></li>
</ul>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<p>use <a class="reference external" href="./run_distributed_eval.py">run_distributed_eval</a>, with the following convenient alias</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deval <span class="o">()</span> <span class="o">{</span>
	<span class="nv">proc</span><span class="o">=</span><span class="nv">$1</span>
	<span class="nv">m</span><span class="o">=</span><span class="nv">$2</span>
	<span class="nv">dd</span><span class="o">=</span><span class="nv">$3</span>
	<span class="nv">sd</span><span class="o">=</span><span class="nv">$4</span>
	<span class="nb">shift</span>
	<span class="nb">shift</span>
	<span class="nb">shift</span>
	<span class="nb">shift</span>
	python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="nv">$proc</span>  run_distributed_eval.py <span class="se">\</span>
		--model_name <span class="nv">$m</span>  --save_dir <span class="nv">$sd</span> --data_dir <span class="nv">$dd</span> <span class="nv">$@</span>
<span class="o">}</span>
</pre></div>
</div>
<p>On a 1 GPU system, here are four commands (that assume <code class="docutils literal notranslate"><span class="pre">xsum</span></code>, <code class="docutils literal notranslate"><span class="pre">cnn_dm</span></code> are downloaded, cmd-F for those links in this file).</p>
<p><code class="docutils literal notranslate"><span class="pre">distilBART</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deval <span class="m">1</span> sshleifer/distilbart-xsum-12-3 xsum dbart_12_3_xsum_eval --fp16  <span class="c1"># --help for more choices.</span>
deval <span class="m">1</span> sshleifer/distilbart-cnn_dm-12-6 cnn_dm dbart_12_6_cnn_eval --fp16
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">distill-pegasus</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deval <span class="m">1</span> sshleifer/distill-pegasus-cnn-16-4 cnn_dm dpx_cnn_eval
deval <span class="m">1</span> sshleifer/distill-pegasus-xsum-16-4 xsum dpx_xsum_eval
</pre></div>
</div>
</div>
<div class="section" id="distillation">
<h2>Distillation<a class="headerlink" href="#distillation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>For all of the following commands, you can get roughly equivalent result and faster run times by passing <code class="docutils literal notranslate"><span class="pre">--num_beams=4</span></code>. That’s not what we did for the paper.</p></li>
<li><p>Besides the KD section, you can also run commands with the built-in transformers trainer. See, for example, <a class="reference external" href="./builtin_trainer/train_distilbart_cnn.sh">builtin_trainer/train_distilbart_cnn.sh</a>.</p></li>
<li><p>Large performance deviations (&gt; 5X slower or more than 0.5 Rouge-2 worse), should be reported.</p></li>
<li><p>Multi-gpu (controlled with <code class="docutils literal notranslate"><span class="pre">--gpus</span></code> should work, but might require more epochs).</p></li>
</ul>
<div class="section" id="recommended-workflow">
<h3>Recommended Workflow<a class="headerlink" href="#recommended-workflow" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Get your dataset in the right format. (see 6 files above).</p></li>
<li><p>Find a teacher model <a class="reference external" href="https://huggingface.co/models?search=pegasus">Pegasus</a> (slower, better ROUGE) or <code class="docutils literal notranslate"><span class="pre">facebook/bart-large-xsum</span></code>/<code class="docutils literal notranslate"><span class="pre">facebook/bart-large-cnn</span></code> (faster, slightly lower.).
Choose the checkpoint where the corresponding dataset is most similar (or identical to) your dataset.</p></li>
<li><p>Follow the sections in order below. You can stop after SFT if you are satisfied, or move on to pseudo-labeling if you want more performance.</p></li>
<li><p>student size: If you want a close to free 50% speedup, cut the decoder in half. If you want a larger speedup, cut it in 4.</p></li>
<li><p>If your SFT run starts at a validation ROUGE-2 that is more than 10 pts below the teacher’s validation ROUGE-2,  you have a bug. Switching to a more expensive technique will not help. Try setting a breakpoint and looking at generation and truncation defaults/hyper-parameters, and share your experience on the forums!</p></li>
</ul>
</div>
<div class="section" id="initialization">
<h3>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h3>
<p>We use <a class="reference external" href="https://github.com/j420247/neural-compressor/blob/ffa9a39cbc19da60a1ff71c88eb57759bcc3e4fa/examples/pytorch/nlp/huggingface_models/common/examples/research_projects/seq2seq-distillation/make_student.py">make_student.py</a> to copy alternating layers from the teacher, and save the resulting model to disk</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python make_student.py facebook/bart-large-xsum --save_path dbart_xsum_12_3  -e <span class="m">12</span> -d <span class="m">3</span>
</pre></div>
</div>
<p>or for <code class="docutils literal notranslate"><span class="pre">pegasus-xsum</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python make_student.py google/pegasus-xsum --save_path dpx_xsum_16_4  --e <span class="m">16</span> --d <span class="m">4</span>
</pre></div>
</div>
<p>we now have an initialized student saved to  <code class="docutils literal notranslate"><span class="pre">dbart_xsum_12_3</span></code>, which we will use for the following commands.</p>
<ul class="simple">
<li><p>Extension: To replicate more complicated initialize experiments in section 6.1, or try your own. Use the <code class="docutils literal notranslate"><span class="pre">create_student_by_copying_alternating_layers</span></code> function.</p></li>
</ul>
</div>
<div class="section" id="pegasus">
<h3>Pegasus<a class="headerlink" href="#pegasus" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The following commands are written for BART and will require, at minimum, the following modifications</p></li>
<li><p>reduce batch size, and increase gradient accumulation steps so that the product <code class="docutils literal notranslate"><span class="pre">gpus</span> <span class="pre">*</span> <span class="pre">batch</span> <span class="pre">size</span> <span class="pre">*</span> <span class="pre">gradient_accumulation_steps</span> <span class="pre">=</span> <span class="pre">256</span></code>. We used <code class="docutils literal notranslate"><span class="pre">--learning-rate</span></code> = 1e-4 * gradient accumulation steps.</p></li>
<li><p>don’t use fp16</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tokenizer_name</span> <span class="pre">google/pegasus-large</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="sft-no-teacher-distillation">
<h2>SFT (No Teacher Distillation)<a class="headerlink" href="#sft-no-teacher-distillation" title="Permalink to this headline">¶</a></h2>
<p>You don’t need <code class="docutils literal notranslate"><span class="pre">distillation.py</span></code>, you can just run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python finetune.py <span class="se">\</span>
  --data_dir xsum <span class="se">\</span>
  --freeze_encoder --freeze_embeds <span class="se">\</span>
  --learning_rate<span class="o">=</span>3e-4 <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_predict <span class="se">\</span>
  --fp16 --fp16_opt_level<span class="o">=</span>O1 <span class="se">\</span>
  --val_check_interval <span class="m">0</span>.1 --n_val <span class="m">1000</span> --eval_beams <span class="m">2</span> --length_penalty<span class="o">=</span><span class="m">0</span>.5 <span class="se">\</span>
  --max_target_length<span class="o">=</span><span class="m">60</span> --val_max_target_length<span class="o">=</span><span class="m">60</span> --test_max_target_length<span class="o">=</span><span class="m">100</span> <span class="se">\</span>
  --model_name_or_path dbart_xsum_12_3 <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="m">64</span> --eval_batch_size<span class="o">=</span><span class="m">64</span> <span class="se">\</span>
  --sortish_sampler <span class="se">\</span>
  --num_train_epochs<span class="o">=</span><span class="m">6</span> <span class="se">\</span>
  --warmup_steps <span class="m">500</span> <span class="se">\</span>
  --output_dir distilbart_xsum_sft_12_3 --gpus <span class="m">1</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note: The command that produced <code class="docutils literal notranslate"><span class="pre">sshleifer/distilbart-cnn-12-6</span></code> is at <a class="reference external" href="./%5Btrain_distilbart_cnn.sh">train_distilbart_cnn.sh</a></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./train_distilbart_cnn.sh
</pre></div>
</div>
<!--- runtime: 6H on NVIDIA RTX 24GB GPU --><ul class="simple">
<li><p>Tip: You can get the same simple distillation logic by using <code class="docutils literal notranslate"><span class="pre">distillation.py</span> <span class="pre">--no_teacher</span> </code> followed by identical arguments as the ones in <code class="docutils literal notranslate"><span class="pre">train_distilbart_cnn.sh</span></code>.
If you are using <code class="docutils literal notranslate"><span class="pre">wandb</span></code> and comparing the two distillation methods, using this entry point will make your logs consistent,
because you will have the same hyper-parameters logged in every run.</p></li>
</ul>
</div>
<div class="section" id="pseudo-labeling">
<h2>Pseudo-Labeling<a class="headerlink" href="#pseudo-labeling" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>You don’t need <code class="docutils literal notranslate"><span class="pre">distillation.py</span></code>.</p></li>
<li><p>Instructions to generate pseudo-labels and use pre-computed pseudo-labels can be found <a class="reference internal" href="precomputed_pseudo_labels.html"><span class="doc">here</span></a>.
Simply run <code class="docutils literal notranslate"><span class="pre">finetune.py</span></code> with one of those pseudo-label datasets as <code class="docutils literal notranslate"><span class="pre">--data_dir</span></code> (<code class="docutils literal notranslate"><span class="pre">DATA</span></code>, below).</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python finetune.py <span class="se">\</span>
  --teacher facebook/bart-large-xsum --data_dir DATA <span class="se">\</span>
  --freeze_encoder --freeze_embeds <span class="se">\</span>
  --learning_rate<span class="o">=</span>3e-4 <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_predict <span class="se">\</span>
  --fp16 --fp16_opt_level<span class="o">=</span>O1 <span class="se">\</span>
  --val_check_interval <span class="m">0</span>.1 --n_val <span class="m">1000</span> --eval_beams <span class="m">2</span> --length_penalty<span class="o">=</span><span class="m">0</span>.5 <span class="se">\</span>
  --max_target_length<span class="o">=</span><span class="m">60</span> --val_max_target_length<span class="o">=</span><span class="m">60</span> --test_max_target_length<span class="o">=</span><span class="m">100</span> <span class="se">\</span>
  --model_name_or_path dbart_xsum_12_3 <span class="se">\</span>
  --train_batch_size<span class="o">=</span><span class="m">32</span> --eval_batch_size<span class="o">=</span><span class="m">32</span> <span class="se">\</span>
  --sortish_sampler <span class="se">\</span>
  --num_train_epochs<span class="o">=</span><span class="m">5</span> <span class="se">\</span>
  --warmup_steps <span class="m">500</span> <span class="se">\</span>
  --output_dir dbart_xsum_12_3_PL --gpus <span class="m">1</span> --logger_name wandb
</pre></div>
</div>
<p>To combine datasets, as in Section 6.2, try something like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl -S https://cdn-datasets.huggingface.co/pseudo/xsum/bart_xsum_pl.tgz <span class="p">|</span> tar -xvz -C .
curl -S https://cdn-datasets.huggingface.co/pseudo/xsum/pegasus_xsum.tgz <span class="p">|</span> tar -xvz -C .
curl -S https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz <span class="p">|</span> tar -xvz -C .
mkdir all_pl
cat bart_xsum_pl/train.source pegasus_xsum/train.source xsum/train.source &gt; all_pl/train.source
cat bart_xsum_pl/train.target pegasus_xsum/train.target xsum/train.target &gt; all_pl/train.target
cp xsum/val* all_pl
cp xsum/test* all_pl
</pre></div>
</div>
<p>then use <code class="docutils literal notranslate"><span class="pre">all_pl</span></code> as DATA in the command above.</p>
<div class="section" id="direct-knowledge-distillation-kd">
<h3>Direct Knowledge Distillation (KD)<a class="headerlink" href="#direct-knowledge-distillation-kd" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In this method, we use try to enforce that the student and teacher produce similar encoder_outputs, logits, and hidden_states using <code class="docutils literal notranslate"><span class="pre">SummarizationDistiller</span></code>.</p></li>
<li><p>This method was used for <code class="docutils literal notranslate"><span class="pre">sshleifer/distilbart-xsum-12-6</span></code>, <code class="docutils literal notranslate"><span class="pre">6-6</span></code>, and <code class="docutils literal notranslate"><span class="pre">9-6</span></code> checkpoints were produced.</p></li>
<li><p>You must use <a class="reference external" href="https://github.com/j420247/neural-compressor/blob/ffa9a39cbc19da60a1ff71c88eb57759bcc3e4fa/examples/pytorch/nlp/huggingface_models/common/examples/research_projects/seq2seq-distillation/distillation.py"><code class="docutils literal notranslate"><span class="pre">distillation.py</span></code></a>. Note that this command initializes the student for you.</p></li>
</ul>
<p>The command that produced <code class="docutils literal notranslate"><span class="pre">sshleifer/distilbart-xsum-12-6</span></code> is at <a class="reference external" href="https://github.com/j420247/neural-compressor/blob/ffa9a39cbc19da60a1ff71c88eb57759bcc3e4fa/examples/pytorch/nlp/huggingface_models/common/examples/research_projects/seq2seq-distillation/train_distilbart_xsum.sh">./train_distilbart_xsum.sh</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./train_distilbart_xsum.sh --logger_name wandb --gpus <span class="m">1</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Expected ROUGE-2 between 21.3 and 21.6, run time ~13H.</p></li>
<li><p>direct KD + Pegasus is VERY slow and works best with <code class="docutils literal notranslate"><span class="pre">--supervise_forward</span> <span class="pre">--normalize_hidden</span></code>.</p></li>
</ul>
<!--- runtime: 13H on V-100 16GB GPU. --></div>
</div>
<div class="section" id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@misc</span><span class="p">{</span><span class="nl">shleifer2020pretrained</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Pre-trained Summarization Distillation}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Sam Shleifer and Alexander M. Rush}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2020}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2010.13002}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
<span class="nc">@article</span><span class="p">{</span><span class="nl">Wolf2019HuggingFacesTS</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{ArXiv}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{abs/1910.03771}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>