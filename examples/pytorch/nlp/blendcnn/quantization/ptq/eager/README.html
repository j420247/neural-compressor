<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Step-by-Step &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Step-by-Step</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/nlp/blendcnn/quantization/ptq/eager/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="step-by-step">
<h1>Step-by-Step<a class="headerlink" href="#step-by-step" title="Permalink to this headline">¶</a></h1>
<p>This document describes the step-by-step instructions for reproducing PyTorch BlendCNN tuning(with MRPC dataset) results with Intel® Neural Compressor.</p>
<blockquote>
<div><p><strong>Note</strong></p>
<p>PyTorch quantization implementation in imperative path has limitation on automatically execution.
It requires to manually add QuantStub and DequantStub for quantizable ops, it also requires to manually do fusion operation.
Intel® Neural Compressor has no capability to solve this framework limitation. Intel® Neural Compressor supposes user have done these two steps before invoking Intel® Neural Compressor interface.
For details, please refer to https://pytorch.org/docs/stable/quantization.html</p>
</div></blockquote>
</div>
<div class="section" id="prerequisite">
<h1>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline">¶</a></h1>
<div class="section" id="installation">
<h2>1. Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/pytorch/nlp/blendcnn/quantization/ptq/eager
pip install -r requirements.txt
pip install <span class="nv">torch</span><span class="o">==</span><span class="m">1</span>.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
</pre></div>
</div>
</div>
<div class="section" id="prepare-model-and-dataset">
<h2>2. Prepare model and Dataset<a class="headerlink" href="#prepare-model-and-dataset" title="Permalink to this headline">¶</a></h2>
<p>Download <a class="reference external" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip">BERT-Base, Uncased</a> and
<a class="reference external" href="https://github.com/nyu-mll/GLUE-baselines">GLUE MRPC Benchmark Datasets</a></p>
<div class="section" id="model">
<h3>model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h3>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>mkdir models/ <span class="o">&amp;&amp;</span> mv uncased_L-12_H-768_A-12.zip models/
<span class="nb">cd</span> models/ <span class="o">&amp;&amp;</span> unzip uncased_L-12_H-768_A-12.zip

<span class="c1"># train blend CNN from scratch</span>
python classify.py --model_config config/blendcnn/mrpc/train.json
</pre></div>
</div>
<p>After below steps, you can find the pre-trained model weights <em><strong>model_final.pt</strong></em> at <code class="docutils literal notranslate"><span class="pre">./models/</span></code></p>
</div>
<div class="section" id="dataset">
<h3>dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h3>
<p>After downloads dataset, you need to put dataset at <code class="docutils literal notranslate"><span class="pre">./MRPC/</span></code>, list this:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>ls MRPC/
dev_ids.tsv  dev.tsv  test.tsv  train.tsv
</pre></div>
</div>
</div>
</div>
<div class="section" id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<div class="section" id="blendcnn">
<h3>blendcnn<a class="headerlink" href="#blendcnn" title="Permalink to this headline">¶</a></h3>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./run_tuning.sh --input_model<span class="o">=</span>/PATH/TO/models/ --dataset_location<span class="o">=</span>/PATH/TO/MRPC/ --output_model<span class="o">=</span>/DIR/TO/INT8_MODEL/

./run_benchmark.sh --int8<span class="o">=</span><span class="nb">true</span> --mode<span class="o">=</span>benchmark --batch_size<span class="o">=</span><span class="m">32</span> --input_model<span class="o">=</span>/DIR/TO/INT8_MODEL/
./run_benchmark.sh --int8<span class="o">=</span>False --mode<span class="o">=</span>benchmark --batch_size<span class="o">=</span><span class="m">32</span> --input_model<span class="o">=</span>/PATH/TO/FP32_MODEL
</pre></div>
</div>
</div>
</div>
<div class="section" id="distillation-of-blendcnn-with-bert-base-as-teacher">
<h2>3. Distillation of BlendCNN with BERT-Base as Teacher<a class="headerlink" href="#distillation-of-blendcnn-with-bert-base-as-teacher" title="Permalink to this headline">¶</a></h2>
<div class="section" id="fine-tune-the-pretrained-bert-base-model-on-mrpc-dataset">
<h3>3.1 Fine-tune the pretrained BERT-Base model on MRPC dataset<a class="headerlink" href="#fine-tune-the-pretrained-bert-base-model-on-mrpc-dataset" title="Permalink to this headline">¶</a></h3>
<p>After preparation of step 2, you can fine-tune the pretrained BERT-Base model on MRPC dataset with below steps.</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>mkdir -p models/bert/mrpc
<span class="c1"># fine-tune the pretrained BERT-Base model</span>
python finetune.py config/finetune/mrpc/train.json
</pre></div>
</div>
<p>When finished, you can find the fine-tuned BERT-Base model weights model_final.pt at <code class="docutils literal notranslate"><span class="pre">./models/bert/mrpc/</span></code>.</p>
</div>
<div class="section" id="distilling-the-blendcnn-with-bert-base">
<h3>3.2 Distilling the BlendCNN with BERT-Base<a class="headerlink" href="#distilling-the-blendcnn-with-bert-base" title="Permalink to this headline">¶</a></h3>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>mkdir -p models/blendcnn/
<span class="c1"># distilling the BlendCNN</span>
python distill.py --loss_weights <span class="m">0</span>.1 <span class="m">0</span>.9
</pre></div>
</div>
<p>Follow the above steps, you will find distilled BlendCNN model weights best_model_weights.pt in <code class="docutils literal notranslate"><span class="pre">./models/blendcnn/</span></code>.</p>
</div>
</div>
</div>
<div class="section" id="examples-of-enabling-intel-neural-compressor-auto-tuning-on-pytorch-resnest">
<h1>Examples of enabling Intel® Neural Compressor auto tuning on PyTorch ResNest<a class="headerlink" href="#examples-of-enabling-intel-neural-compressor-auto-tuning-on-pytorch-resnest" title="Permalink to this headline">¶</a></h1>
<p>This is a tutorial of how to enable a PyTorch classification model with Intel® Neural Compressor.</p>
<div class="section" id="user-code-analysis">
<h2>User Code Analysis<a class="headerlink" href="#user-code-analysis" title="Permalink to this headline">¶</a></h2>
<p>Intel® Neural Compressor supports three usages:</p>
<ol class="simple">
<li><p>User only provide fp32 “model”, and configure calibration dataset, evaluation dataset and metric in model-specific yaml config file.</p></li>
<li><p>User provide fp32 “model”, calibration dataset “q_dataloader” and evaluation dataset “eval_dataloader”, and configure metric in tuning.metric field of model-specific yaml config file.</p></li>
<li><p>User specifies fp32 “model”, calibration dataset “q_dataloader” and a custom “eval_func” which encapsulates the evaluation dataset and metric by itself.</p></li>
</ol>
<p>As ResNest series are typical classification models, use Top-K as metric which is built-in supported by Intel® Neural Compressor. So here we integrate PyTorch ResNest with Intel® Neural Compressor by the first use case for simplicity.</p>
<div class="section" id="write-yaml-config-file">
<h3>Write Yaml config file<a class="headerlink" href="#write-yaml-config-file" title="Permalink to this headline">¶</a></h3>
<p>In examples directory, there is a template.yaml. We could remove most of items and only keep mandatory item for tuning.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>                                               <span class="c1"># mandatory. used to specify model specific information.</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">blendcnn</span>
  <span class="n">framework</span><span class="p">:</span> <span class="n">pytorch</span>     

<span class="n">tuning</span><span class="p">:</span>
  <span class="n">accuracy_criterion</span><span class="p">:</span>
    <span class="n">relative</span><span class="p">:</span>  <span class="mf">0.01</span>                                  <span class="c1"># optional. default value is relative, other value is absolute. this example allows relative accuracy loss: 1%.</span>
  <span class="n">exit_policy</span><span class="p">:</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="mi">0</span>                                       <span class="c1"># optional. tuning timeout (seconds). default value is 0 which means early stop. combine with max_trials field to decide when to exit.</span>
  <span class="n">random_seed</span><span class="p">:</span> <span class="mi">9527</span>                                  <span class="c1"># optional. random seed for deterministic tuning.</span>
</pre></div>
</div>
<p>Here we use specifies fp32 “model”, calibration dataset “q_dataloader” and a custom “eval_func” which encapsulates the evaluation dataset and metric.</p>
</div>
<div class="section" id="prepare">
<h3>prepare<a class="headerlink" href="#prepare" title="Permalink to this headline">¶</a></h3>
<p>PyTorch quantization requires two manual steps:</p>
<ol class="simple">
<li><p>Add QuantStub and DeQuantStub for all quantizable ops.</p></li>
<li><p>Fuse possible patterns, such as Conv + Relu and Conv + BN + Relu.</p></li>
</ol>
<p>It’s intrinsic limitation of PyTorch quantization imperative path. No way to develop a code to automatically do that.(Please refer <a class="reference external" href="https://github.com/j420247/neural-compressor/blob/ffa9a39cbc19da60a1ff71c88eb57759bcc3e4fa/examples/pytorch/nlp/blendcnn/quantization/ptq/eager/./models.py">sample code</a>)</p>
</div>
<div class="section" id="code-update">
<h3>code update<a class="headerlink" href="#code-update" title="Permalink to this headline">¶</a></h3>
<p>After prepare step is done, we just need update classify.py like below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Quantization</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">Bert_DataLoader</span><span class="p">(</span><span class="n">loader</span><span class="o">=</span><span class="n">data_iter</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">nc_yaml</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">dataloader</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">eval_func</span> <span class="o">=</span> <span class="n">eval_func</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>The quantizer.fit() function will return a best quantized model during timeout constrain.(Please refer <a class="reference external" href="https://github.com/j420247/neural-compressor/blob/ffa9a39cbc19da60a1ff71c88eb57759bcc3e4fa/examples/pytorch/nlp/blendcnn/quantization/ptq/eager/./classify.py">sample code</a>)</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>