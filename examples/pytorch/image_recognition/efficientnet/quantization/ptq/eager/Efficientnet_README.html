<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>(Generic) EfficientNets for PyTorch &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../../../../../" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../../../../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
              <div class="version">
                1.14.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">(Generic) EfficientNets for PyTorch</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/examples/pytorch/image_recognition/efficientnet/quantization/ptq/eager/Efficientnet_README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="generic-efficientnets-for-pytorch">
<h1>(Generic) EfficientNets for PyTorch<a class="headerlink" href="#generic-efficientnets-for-pytorch" title="Permalink to this headline">¶</a></h1>
<p>A ‘generic’ implementation of EfficientNet, MixNet, MobileNetV3, etc. that covers most of the compute/parameter efficient architectures derived from the MobileNet V1/V2 block sequence, including those found via automated neural architecture search.</p>
<p>All models are implemented by GenEfficientNet or MobileNetV3 classes, with string based architecture definitions to configure the block layouts (idea from <a class="reference external" href="https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_models.py">here</a>)</p>
<div class="section" id="what-s-new">
<h2>What’s New<a class="headerlink" href="#what-s-new" title="Permalink to this headline">¶</a></h2>
<div class="section" id="april-5-2020">
<h3>April 5, 2020<a class="headerlink" href="#april-5-2020" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite</p>
<ul>
<li><p>3.5M param MobileNet-V2 100 &#64; 73%</p></li>
<li><p>4.5M param MobileNet-V2 110d &#64; 75%</p></li>
<li><p>6.1M param MobileNet-V2 140 &#64; 76.5%</p></li>
<li><p>5.8M param MobileNet-V2 120d &#64; 77.3%</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="march-23-2020">
<h3>March 23, 2020<a class="headerlink" href="#march-23-2020" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add EfficientNet-Lite models w/ weights ported from <a class="reference external" href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite">Tensorflow TPU</a></p></li>
<li><p>Add PyTorch trained MobileNet-V3 Large weights with 75.77% top-1</p></li>
<li><p>IMPORTANT CHANGE (if training from scratch) - weight init changed to better match Tensorflow impl, set <code class="docutils literal notranslate"><span class="pre">fix_group_fanout=False</span></code> in <code class="docutils literal notranslate"><span class="pre">initialize_weight_goog</span></code> for old behavior</p></li>
</ul>
</div>
<div class="section" id="feb-12-2020">
<h3>Feb 12, 2020<a class="headerlink" href="#feb-12-2020" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from <a class="reference external" href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet">Tensorflow TPU</a></p></li>
<li><p>Port new EfficientNet-B8 (RandAugment) weights from TF TPU, these are different than the B8 AdvProp, different input normalization.</p></li>
<li><p>Add RandAugment PyTorch trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by <a class="reference external" href="https://github.com/andravin">Andrew Lavin</a></p></li>
</ul>
</div>
<div class="section" id="jan-22-2020">
<h3>Jan 22, 2020<a class="headerlink" href="#jan-22-2020" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Update weights for EfficientNet B0, B2, B3 and MixNet-XL with latest RandAugment trained weights. Trained with (https://github.com/rwightman/pytorch-image-models)</p></li>
<li><p>Fix torchscript compatibility for PyTorch 1.4, add torchscript support for MixedConv2d using ModuleDict</p></li>
<li><p>Test models, torchscript, onnx export with PyTorch 1.4 – no issues</p></li>
</ul>
</div>
<div class="section" id="nov-22-2019">
<h3>Nov 22, 2019<a class="headerlink" href="#nov-22-2019" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>New top-1 high! Ported official TF EfficientNet AdvProp (https://arxiv.org/abs/1911.09665) weights and B8 model spec. Created a new set of <code class="docutils literal notranslate"><span class="pre">ap</span></code> models since they use a different
preprocessing (Inception mean/std) from the original EfficientNet base/AA/RA weights.</p></li>
</ul>
</div>
<div class="section" id="nov-15-2019">
<h3>Nov 15, 2019<a class="headerlink" href="#nov-15-2019" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Ported official TF MobileNet-V3 float32 large/small/minimalistic weights</p></li>
<li><p>Modifications to MobileNet-V3 model and components to support some additional config needed for differences between TF MobileNet-V3 and mine</p></li>
</ul>
</div>
<div class="section" id="oct-30-2019">
<h3>Oct 30, 2019<a class="headerlink" href="#oct-30-2019" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Many of the models will now work with torch.jit.script, MixNet being the biggest exception</p></li>
<li><p>Improved interface for enabling torchscript or ONNX export compatible modes (via config)</p></li>
<li><p>Add JIT optimized mem-efficient Swish/Mish autograd.fn in addition to memory-efficient autgrad.fn</p></li>
<li><p>Activation factory to select best version of activation by name or override one globally</p></li>
<li><p>Add pretrained checkpoint load helper that handles input conv and classifier changes</p></li>
</ul>
</div>
<div class="section" id="oct-27-2019">
<h3>Oct 27, 2019<a class="headerlink" href="#oct-27-2019" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add CondConv EfficientNet variants ported from https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv</p></li>
<li><p>Add RandAug weights for TF EfficientNet B5 and B7 from https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</p></li>
<li><p>Bring over MixNet-XL model and depth scaling algo from my pytorch-image-models code base</p></li>
<li><p>Switch activations and global pooling to modules</p></li>
<li><p>Add memory-efficient Swish/Mish impl</p></li>
<li><p>Add as_sequential() method to all models and allow as an argument in entrypoint fns</p></li>
<li><p>Move MobileNetV3 into own file since it has a different head</p></li>
<li><p>Remove ChamNet, MobileNet V2/V1 since they will likely never be used here</p></li>
</ul>
</div>
</div>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p>Implemented models include:</p>
<ul class="simple">
<li><p>EfficientNet NoisyStudent (B0-B7, L2) (https://arxiv.org/abs/1911.04252)</p></li>
<li><p>EfficientNet AdvProp (B0-B8) (https://arxiv.org/abs/1911.09665)</p></li>
<li><p>EfficientNet (B0-B8) (https://arxiv.org/abs/1905.11946)</p></li>
<li><p>EfficientNet-EdgeTPU (S, M, L) (https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html)</p></li>
<li><p>EfficientNet-CondConv (https://arxiv.org/abs/1904.04971)</p></li>
<li><p>EfficientNet-Lite (https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite)</p></li>
<li><p>MixNet (https://arxiv.org/abs/1907.09595)</p></li>
<li><p>MNASNet B1, A1 (Squeeze-Excite), and Small (https://arxiv.org/abs/1807.11626)</p></li>
<li><p>MobileNet-V3 (https://arxiv.org/abs/1905.02244)</p></li>
<li><p>FBNet-C (https://arxiv.org/abs/1812.03443)</p></li>
<li><p>Single-Path NAS (https://arxiv.org/abs/1904.02877)</p></li>
</ul>
<p>I originally implemented and trained some these models with code <a class="reference external" href="https://github.com/rwightman/pytorch-image-models">here</a>, this repository contains just the GenEfficientNet models, validation, and associated ONNX/Caffe2 export code.</p>
</div>
<div class="section" id="pretrained">
<h2>Pretrained<a class="headerlink" href="#pretrained" title="Permalink to this headline">¶</a></h2>
<p>I’ve managed to train several of the models to accuracies close to or above the originating papers and official impl. My training code is here: https://github.com/rwightman/pytorch-image-models</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Prec@1 (Err)</th>
<th>Prec@5 (Err)</th>
<th>Param#(M)</th>
<th>MAdds(M)</th>
<th>Image Scaling</th>
<th>Resolution</th>
<th>Crop</th>
</tr>
</thead>
<tbody>
<tr>
<td>efficientnet_b3</td>
<td>81.866 (18.134)</td>
<td>95.836 (4.164)</td>
<td>12.23</td>
<td>TBD</td>
<td>bicubic</td>
<td>320</td>
<td>1.0</td>
</tr>
<tr>
<td>efficientnet_b3</td>
<td>81.508 (18.492)</td>
<td>95.672 (4.328)</td>
<td>12.23</td>
<td>TBD</td>
<td>bicubic</td>
<td>300</td>
<td>0.904</td>
</tr>
<tr>
<td>mixnet_xl</td>
<td>81.074 (18.926)</td>
<td>95.282 (4.718)</td>
<td>11.90</td>
<td>TBD</td>
<td>bicubic</td>
<td>256</td>
<td>1.0</td>
</tr>
<tr>
<td>efficientnet_b2</td>
<td>80.612 (19.388)</td>
<td>95.318 (4.682)</td>
<td>9.1</td>
<td>TBD</td>
<td>bicubic</td>
<td>288</td>
<td>1.0</td>
</tr>
<tr>
<td>mixnet_xl</td>
<td>80.476 (19.524)</td>
<td>94.936 (5.064)</td>
<td>11.90</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>efficientnet_b2</td>
<td>80.288 (19.712)</td>
<td>95.166 (4.834)</td>
<td>9.1</td>
<td>1003</td>
<td>bicubic</td>
<td>260</td>
<td>0.890</td>
</tr>
<tr>
<td>mixnet_l</td>
<td>78.976 (21.024</td>
<td>94.184 (5.816)</td>
<td>7.33</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>efficientnet_b1</td>
<td>78.692 (21.308)</td>
<td>94.086 (5.914)</td>
<td>7.8</td>
<td>694</td>
<td>bicubic</td>
<td>240</td>
<td>0.882</td>
</tr>
<tr>
<td>efficientnet_es</td>
<td>78.066 (21.934)</td>
<td>93.926 (6.074)</td>
<td>5.44</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>efficientnet_b0</td>
<td>77.698 (22.302)</td>
<td>93.532 (6.468)</td>
<td>5.3</td>
<td>390</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv2_120d</td>
<td>77.294 (22.706</td>
<td>93.502 (6.498)</td>
<td>5.8</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mixnet_m</td>
<td>77.256 (22.744)</td>
<td>93.418 (6.582)</td>
<td>5.01</td>
<td>353</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv2_140</td>
<td>76.524 (23.476)</td>
<td>92.990 (7.010)</td>
<td>6.1</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mixnet_s</td>
<td>75.988 (24.012)</td>
<td>92.794 (7.206)</td>
<td>4.13</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv3_large_100</td>
<td>75.766 (24.234)</td>
<td>92.542 (7.458)</td>
<td>5.5</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv3_rw</td>
<td>75.634 (24.366)</td>
<td>92.708 (7.292)</td>
<td>5.5</td>
<td>219</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mnasnet_a1</td>
<td>75.448 (24.552)</td>
<td>92.604 (7.396)</td>
<td>3.9</td>
<td>312</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>fbnetc_100</td>
<td>75.124 (24.876)</td>
<td>92.386 (7.614)</td>
<td>5.6</td>
<td>385</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv2_110d</td>
<td>75.052 (24.948)</td>
<td>92.180 (7.820)</td>
<td>4.5</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mnasnet_b1</td>
<td>74.658 (25.342)</td>
<td>92.114 (7.886)</td>
<td>4.4</td>
<td>315</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>spnasnet_100</td>
<td>74.084 (25.916)</td>
<td>91.818 (8.182)</td>
<td>4.4</td>
<td>TBD</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>mobilenetv2_100</td>
<td>72.978 (27.022)</td>
<td>91.016 (8.984)</td>
<td>3.5</td>
<td>TBD</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
</tbody>
</table><p>More pretrained models to come…</p>
</div>
<div class="section" id="ported-weights">
<h2>Ported Weights<a class="headerlink" href="#ported-weights" title="Permalink to this headline">¶</a></h2>
<p>The weights ported from Tensorflow checkpoints for the EfficientNet models do pretty much match accuracy in Tensorflow once a SAME convolution padding equivalent is added, and the same crop factors, image scaling, etc (see table) are used via cmd line args.</p>
<p><strong>IMPORTANT:</strong></p>
<ul class="simple">
<li><p>Tensorflow ported weights for EfficientNet AdvProp (AP), EfficientNet EdgeTPU, EfficientNet-CondConv, EfficientNet-Lite, and MobileNet-V3 models use Inception style (0.5, 0.5, 0.5) for mean and std.</p></li>
<li><p>Enabling the Tensorflow preprocessing pipeline with <code class="docutils literal notranslate"><span class="pre">--tf-preprocessing</span></code> at validation time will improve scores by 0.1-0.5%, very close to original TF impl.</p></li>
</ul>
<p>To run validation for tf_efficientnet_b5:
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">validate.py</span> <span class="pre">/path/to/imagenet/validation/</span> <span class="pre">--model</span> <span class="pre">tf_efficientnet_b5</span> <span class="pre">-b</span> <span class="pre">64</span> <span class="pre">--img-size</span> <span class="pre">456</span> <span class="pre">--crop-pct</span> <span class="pre">0.934</span> <span class="pre">--interpolation</span> <span class="pre">bicubic</span></code></p>
<p>To run validation w/ TF preprocessing for tf_efficientnet_b5:
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">validate.py</span> <span class="pre">/path/to/imagenet/validation/</span> <span class="pre">--model</span> <span class="pre">tf_efficientnet_b5</span> <span class="pre">-b</span> <span class="pre">64</span> <span class="pre">--img-size</span> <span class="pre">456</span> <span class="pre">--tf-preprocessing</span></code></p>
<p>To run validation for a model with Inception preprocessing, ie EfficientNet-B8 AdvProp:
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">validate.py</span> <span class="pre">/path/to/imagenet/validation/</span> <span class="pre">--model</span> <span class="pre">tf_efficientnet_b8_ap</span> <span class="pre">-b</span> <span class="pre">48</span> <span class="pre">--num-gpu</span> <span class="pre">2</span> <span class="pre">--img-size</span> <span class="pre">672</span> <span class="pre">--crop-pct</span> <span class="pre">0.954</span> <span class="pre">--mean</span> <span class="pre">0.5</span> <span class="pre">--std</span> <span class="pre">0.5</span></code></p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Prec@1 (Err)</th>
<th>Prec@5 (Err)</th>
<th>Param #</th>
<th>Image Scaling</th>
<th>Image Size</th>
<th>Crop</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf_efficientnet_l2_ns *tfp</td>
<td>88.352 (11.648)</td>
<td>98.652 (1.348)</td>
<td>480</td>
<td>bicubic</td>
<td>800</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_l2_ns</td>
<td>TBD</td>
<td>TBD</td>
<td>480</td>
<td>bicubic</td>
<td>800</td>
<td>0.961</td>
</tr>
<tr>
<td>tf_efficientnet_l2_ns_475</td>
<td>88.234 (11.766)</td>
<td>98.546 (1.454)</td>
<td>480</td>
<td>bicubic</td>
<td>475</td>
<td>0.936</td>
</tr>
<tr>
<td>tf_efficientnet_l2_ns_475 *tfp</td>
<td>88.172 (11.828)</td>
<td>98.566 (1.434)</td>
<td>480</td>
<td>bicubic</td>
<td>475</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b7_ns *tfp</td>
<td>86.844 (13.156)</td>
<td>98.084 (1.916)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b7_ns</td>
<td>86.840 (13.160)</td>
<td>98.094 (1.906)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b6_ns</td>
<td>86.452 (13.548)</td>
<td>97.882 (2.118)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b6_ns *tfp</td>
<td>86.444 (13.556)</td>
<td>97.880 (2.120)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b5_ns *tfp</td>
<td>86.064 (13.936)</td>
<td>97.746 (2.254)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b5_ns</td>
<td>86.088 (13.912)</td>
<td>97.752 (2.248)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b8_ap *tfp</td>
<td>85.436 (14.564)</td>
<td>97.272 (2.728)</td>
<td>87.4</td>
<td>bicubic</td>
<td>672</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b8 *tfp</td>
<td>85.384 (14.616)</td>
<td>97.394 (2.606)</td>
<td>87.4</td>
<td>bicubic</td>
<td>672</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b8</td>
<td>85.370 (14.630)</td>
<td>97.390 (2.610)</td>
<td>87.4</td>
<td>bicubic</td>
<td>672</td>
<td>0.954</td>
</tr>
<tr>
<td>tf_efficientnet_b8_ap</td>
<td>85.368 (14.632)</td>
<td>97.294 (2.706)</td>
<td>87.4</td>
<td>bicubic</td>
<td>672</td>
<td>0.954</td>
</tr>
<tr>
<td>tf_efficientnet_b4_ns *tfp</td>
<td>85.298 (14.702)</td>
<td>97.504 (2.496)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b4_ns</td>
<td>85.162 (14.838)</td>
<td>97.470 (2.530)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>0.922</td>
</tr>
<tr>
<td>tf_efficientnet_b7_ap *tfp</td>
<td>85.154 (14.846)</td>
<td>97.244 (2.756)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b7_ap</td>
<td>85.118 (14.882)</td>
<td>97.252 (2.748)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>0.949</td>
</tr>
<tr>
<td>tf_efficientnet_b7 *tfp</td>
<td>84.940 (15.060)</td>
<td>97.214 (2.786)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b7</td>
<td>84.932 (15.068)</td>
<td>97.208 (2.792)</td>
<td>66.35</td>
<td>bicubic</td>
<td>600</td>
<td>0.949</td>
</tr>
<tr>
<td>tf_efficientnet_b6_ap</td>
<td>84.786 (15.214)</td>
<td>97.138 (2.862)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>0.942</td>
</tr>
<tr>
<td>tf_efficientnet_b6_ap *tfp</td>
<td>84.760 (15.240)</td>
<td>97.124 (2.876)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b5_ap *tfp</td>
<td>84.276 (15.724)</td>
<td>96.932 (3.068)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b5_ap</td>
<td>84.254 (15.746)</td>
<td>96.976 (3.024)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>0.934</td>
</tr>
<tr>
<td>tf_efficientnet_b6 *tfp</td>
<td>84.140 (15.860)</td>
<td>96.852 (3.148)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b6</td>
<td>84.110 (15.890)</td>
<td>96.886 (3.114)</td>
<td>43.04</td>
<td>bicubic</td>
<td>528</td>
<td>0.942</td>
</tr>
<tr>
<td>tf_efficientnet_b3_ns *tfp</td>
<td>84.054 (15.946)</td>
<td>96.918 (3.082)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b3_ns</td>
<td>84.048 (15.952)</td>
<td>96.910 (3.090)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>.904</td>
</tr>
<tr>
<td>tf_efficientnet_b5 *tfp</td>
<td>83.822 (16.178)</td>
<td>96.756 (3.244)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b5</td>
<td>83.812 (16.188)</td>
<td>96.748 (3.252)</td>
<td>30.39</td>
<td>bicubic</td>
<td>456</td>
<td>0.934</td>
</tr>
<tr>
<td>tf_efficientnet_b4_ap *tfp</td>
<td>83.278 (16.722)</td>
<td>96.376 (3.624)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b4_ap</td>
<td>83.248 (16.752)</td>
<td>96.388 (3.612)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>0.922</td>
</tr>
<tr>
<td>tf_efficientnet_b4</td>
<td>83.022 (16.978)</td>
<td>96.300 (3.700)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>0.922</td>
</tr>
<tr>
<td>tf_efficientnet_b4 *tfp</td>
<td>82.948 (17.052)</td>
<td>96.308 (3.692)</td>
<td>19.34</td>
<td>bicubic</td>
<td>380</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b2_ns *tfp</td>
<td>82.436 (17.564)</td>
<td>96.268 (3.732)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b2_ns</td>
<td>82.380 (17.620)</td>
<td>96.248 (3.752)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>0.89</td>
</tr>
<tr>
<td>tf_efficientnet_b3_ap *tfp</td>
<td>81.882 (18.118)</td>
<td>95.662 (4.338)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b3_ap</td>
<td>81.828 (18.172)</td>
<td>95.624 (4.376)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>0.904</td>
</tr>
<tr>
<td>tf_efficientnet_b3</td>
<td>81.636 (18.364)</td>
<td>95.718 (4.282)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>0.904</td>
</tr>
<tr>
<td>tf_efficientnet_b3 *tfp</td>
<td>81.576 (18.424)</td>
<td>95.662 (4.338)</td>
<td>12.23</td>
<td>bicubic</td>
<td>300</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite4</td>
<td>81.528 (18.472)</td>
<td>95.668 (4.332)</td>
<td>13.00</td>
<td>bilinear</td>
<td>380</td>
<td>0.92</td>
</tr>
<tr>
<td>tf_efficientnet_b1_ns *tfp</td>
<td>81.514 (18.486)</td>
<td>95.776 (4.224)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite4 *tfp</td>
<td>81.502 (18.498)</td>
<td>95.676 (4.324)</td>
<td>13.00</td>
<td>bilinear</td>
<td>380</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b1_ns</td>
<td>81.388 (18.612)</td>
<td>95.738 (4.262)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>0.88</td>
</tr>
<tr>
<td>tf_efficientnet_el</td>
<td>80.534 (19.466)</td>
<td>95.190 (4.810)</td>
<td>10.59</td>
<td>bicubic</td>
<td>300</td>
<td>0.904</td>
</tr>
<tr>
<td>tf_efficientnet_el *tfp</td>
<td>80.476 (19.524)</td>
<td>95.200 (4.800)</td>
<td>10.59</td>
<td>bicubic</td>
<td>300</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b2_ap *tfp</td>
<td>80.420 (19.580)</td>
<td>95.040 (4.960)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b2_ap</td>
<td>80.306 (19.694)</td>
<td>95.028 (4.972)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>0.890</td>
</tr>
<tr>
<td>tf_efficientnet_b2 *tfp</td>
<td>80.188 (19.812)</td>
<td>94.974 (5.026)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b2</td>
<td>80.086 (19.914)</td>
<td>94.908 (5.092)</td>
<td>9.11</td>
<td>bicubic</td>
<td>260</td>
<td>0.890</td>
</tr>
<tr>
<td>tf_efficientnet_lite3</td>
<td>79.812 (20.188)</td>
<td>94.914 (5.086)</td>
<td>8.20</td>
<td>bilinear</td>
<td>300</td>
<td>0.904</td>
</tr>
<tr>
<td>tf_efficientnet_lite3 *tfp</td>
<td>79.734 (20.266)</td>
<td>94.838 (5.162)</td>
<td>8.20</td>
<td>bilinear</td>
<td>300</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b1_ap *tfp</td>
<td>79.532 (20.468)</td>
<td>94.378 (5.622)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b1_8e *tfp</td>
<td>79.464 (20.536)</td>
<td>94.492 (5.508)</td>
<td>39.7</td>
<td>bicubic</td>
<td>240</td>
<td>0.88</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b1_8e</td>
<td>79.298 (20.702)</td>
<td>94.364 (5.636)</td>
<td>39.7</td>
<td>bicubic</td>
<td>240</td>
<td>0.88</td>
</tr>
<tr>
<td>tf_efficientnet_b1_ap</td>
<td>79.278 (20.722)</td>
<td>94.308 (5.692)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>0.88</td>
</tr>
<tr>
<td>tf_efficientnet_b1 *tfp</td>
<td>79.172 (20.828)</td>
<td>94.450 (5.550)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_em *tfp</td>
<td>78.958 (21.042)</td>
<td>94.458 (5.542)</td>
<td>6.90</td>
<td>bicubic</td>
<td>240</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b0_ns *tfp</td>
<td>78.806 (21.194)</td>
<td>94.496 (5.504)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mixnet_l *tfp</td>
<td>78.846 (21.154)</td>
<td>94.212 (5.788)</td>
<td>7.33</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b1</td>
<td>78.826 (21.174)</td>
<td>94.198 (5.802)</td>
<td>7.79</td>
<td>bicubic</td>
<td>240</td>
<td>0.88</td>
</tr>
<tr>
<td>tf_mixnet_l</td>
<td>78.770 (21.230)</td>
<td>94.004 (5.996)</td>
<td>7.33</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_em</td>
<td>78.742 (21.258)</td>
<td>94.332 (5.668)</td>
<td>6.90</td>
<td>bicubic</td>
<td>240</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_b0_ns</td>
<td>78.658 (21.342)</td>
<td>94.376 (5.624)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b0_8e *tfp</td>
<td>78.314 (21.686)</td>
<td>93.790 (6.210)</td>
<td>24.0</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b0_8e</td>
<td>77.908 (22.092)</td>
<td>93.656 (6.344)</td>
<td>24.0</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b0_4e *tfp</td>
<td>77.746 (22.254)</td>
<td>93.552 (6.448)</td>
<td>13.3</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_cc_b0_4e</td>
<td>77.304 (22.696)</td>
<td>93.332 (6.668)</td>
<td>13.3</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_es *tfp</td>
<td>77.616 (22.384)</td>
<td>93.750 (6.250)</td>
<td>5.44</td>
<td>bicubic</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite2 *tfp</td>
<td>77.544 (22.456)</td>
<td>93.800 (6.200)</td>
<td>6.09</td>
<td>bilinear</td>
<td>260</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite2</td>
<td>77.460 (22.540)</td>
<td>93.746 (6.254)</td>
<td>6.09</td>
<td>bicubic</td>
<td>260</td>
<td>0.89</td>
</tr>
<tr>
<td>tf_efficientnet_b0_ap *tfp</td>
<td>77.514 (22.486)</td>
<td>93.576 (6.424)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_es</td>
<td>77.264 (22.736)</td>
<td>93.600 (6.400)</td>
<td>5.44</td>
<td>bicubic</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b0 *tfp</td>
<td>77.258 (22.742)</td>
<td>93.478 (6.522)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_b0_ap</td>
<td>77.084 (22.916)</td>
<td>93.254 (6.746)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mixnet_m *tfp</td>
<td>77.072 (22.928)</td>
<td>93.368 (6.632)</td>
<td>5.01</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mixnet_m</td>
<td>76.950 (23.050)</td>
<td>93.156 (6.844)</td>
<td>5.01</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_b0</td>
<td>76.848 (23.152)</td>
<td>93.228 (6.772)</td>
<td>5.29</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_lite1 *tfp</td>
<td>76.764 (23.236)</td>
<td>93.326 (6.674)</td>
<td>5.42</td>
<td>bilinear</td>
<td>240</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite1</td>
<td>76.638 (23.362)</td>
<td>93.232 (6.768)</td>
<td>5.42</td>
<td>bicubic</td>
<td>240</td>
<td>0.882</td>
</tr>
<tr>
<td>tf_mixnet_s *tfp</td>
<td>75.800 (24.200)</td>
<td>92.788 (7.212)</td>
<td>4.13</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_100 *tfp</td>
<td>75.768 (24.232)</td>
<td>92.710 (7.290)</td>
<td>5.48</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mixnet_s</td>
<td>75.648 (24.352)</td>
<td>92.636 (7.364)</td>
<td>4.13</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_100</td>
<td>75.516 (24.484)</td>
<td>92.600 (7.400)</td>
<td>5.48</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_efficientnet_lite0 *tfp</td>
<td>75.074 (24.926)</td>
<td>92.314 (7.686)</td>
<td>4.65</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_efficientnet_lite0</td>
<td>74.842 (25.158)</td>
<td>92.170 (7.830)</td>
<td>4.65</td>
<td>bicubic</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_075 *tfp</td>
<td>73.730 (26.270)</td>
<td>91.616 (8.384)</td>
<td>3.99</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_075</td>
<td>73.442 (26.558)</td>
<td>91.352 (8.648)</td>
<td>3.99</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_minimal_100 *tfp</td>
<td>72.678 (27.322)</td>
<td>90.860 (9.140)</td>
<td>3.92</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_large_minimal_100</td>
<td>72.244 (27.756)</td>
<td>90.636 (9.364)</td>
<td>3.92</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_100 *tfp</td>
<td>67.918 (32.082)</td>
<td>87.958 (12.042</td>
<td>2.54</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_100</td>
<td>67.918 (32.082)</td>
<td>87.662 (12.338)</td>
<td>2.54</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_075 *tfp</td>
<td>66.142 (33.858)</td>
<td>86.498 (13.502)</td>
<td>2.04</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_075</td>
<td>65.718 (34.282)</td>
<td>86.136 (13.864)</td>
<td>2.04</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_minimal_100 *tfp</td>
<td>63.378 (36.622)</td>
<td>84.802 (15.198)</td>
<td>2.04</td>
<td>bilinear</td>
<td>224</td>
<td>N/A</td>
</tr>
<tr>
<td>tf_mobilenetv3_small_minimal_100</td>
<td>62.898 (37.102)</td>
<td>84.230 (15.770)</td>
<td>2.04</td>
<td>bilinear</td>
<td>224</td>
<td>0.875</td>
</tr>
</tbody>
</table><p>*tfp models validated with <code class="docutils literal notranslate"><span class="pre">tf-preprocessing</span></code> pipeline</p>
<p>Google tf and tflite weights ported from official Tensorflow repositories</p>
<ul class="simple">
<li><p>https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet</p></li>
<li><p>https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</p></li>
<li><p>https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</p></li>
</ul>
</div>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="environment">
<h3>Environment<a class="headerlink" href="#environment" title="Permalink to this headline">¶</a></h3>
<p>All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically Python 3.6.x and 3.7.x.</p>
<p>Users have reported that a Python 3 Anaconda install in Windows works. I have not verified this myself.</p>
<p>PyTorch versions 1.2. 1.3.1, 1.4 have been tested with this code.</p>
<p>I’ve tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda create -n torch-env
conda activate torch-env
conda install -c pytorch pytorch torchvision <span class="nv">cudatoolkit</span><span class="o">=</span><span class="m">10</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-hub">
<h3>PyTorch Hub<a class="headerlink" href="#pytorch-hub" title="Permalink to this headline">¶</a></h3>
<p>Models can be accessed via the PyTorch Hub API</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="s1">&#39;rwightman/gen-efficientnet-pytorch&#39;</span><span class="p">)</span>
<span class="go">[&#39;efficientnet_b0&#39;, ...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;rwightman/gen-efficientnet-pytorch&#39;</span><span class="p">,</span> <span class="s1">&#39;efficientnet_b0&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="pip">
<h3>Pip<a class="headerlink" href="#pip" title="Permalink to this headline">¶</a></h3>
<p>This package can be installed via pip.</p>
<p>Install (after conda env/install):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install geffnet
</pre></div>
</div>
<p>Eval use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">geffnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">geffnet</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">&#39;mobilenetv3_rw&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<p>Train use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">geffnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># models can also be created by using the entrypoint directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">geffnet</span><span class="o">.</span><span class="n">efficientnet_b2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">drop_connect_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>Create in a nn.Sequential container, for fast.ai, etc:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">geffnet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">geffnet</span><span class="o">.</span><span class="n">mixnet_l</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">drop_connect_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">as_sequential</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="exporting">
<h3>Exporting<a class="headerlink" href="#exporting" title="Permalink to this headline">¶</a></h3>
<p>Scripts to export models to ONNX and then to Caffe2 are included, along with a Caffe2 script to verify.</p>
<p>As an example, to export the MobileNet-V3 pretrained model and then run an Imagenet validation:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python onnx_export.py --model tf_mobilenetv3_large_100 ./mobilenetv3_100.onnx
python onnx_optimize.py ./mobilenetv3_100.onnx --output ./mobilenetv3_100-opt.onnx
python onnx_to_caffe.py ./mobilenetv3_100-opt.onnx --c2-prefix mobilenetv3
python caffe2_validate.py /imagenet/validation/ --c2-init ./mobilenetv3.init.pb --c2-predict ./mobilenetv3.predict.pb --interpolation bicubic
</pre></div>
</div>
<p><strong>NOTE</strong> the TF ported weights with the ‘SAME’ conv padding activated cannot be exported to ONNX unless <code class="docutils literal notranslate"><span class="pre">_EXPORTABLE</span></code> flag in <code class="docutils literal notranslate"><span class="pre">config.py</span></code> is set to True. Use <code class="docutils literal notranslate"><span class="pre">config.set_exportable(True)</span></code> as in the updated <code class="docutils literal notranslate"><span class="pre">onnx_export.py</span></code> example script.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>